{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd50a6e0-338d-4fba-b4c5-7477bb0c56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/JiahuiYu/neuralgym\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CARLA_DATA = True\n",
    "\n",
    "# dataset_dir = \"/home/summer2025/AVPrivacy/carla_sim/test_images2/\"\n",
    "dataset_dir = \"/home/jetsonuser/masking/datasets\"\n",
    "\n",
    "if CARLA_DATA:\n",
    "    num_views = 1\n",
    "    num_frames = 179\n",
    "    # num_frames = 10\n",
    "    dataset_idx = 0\n",
    "    MASK_SPAN = {\"face\":1, \"no_feet\":1, \"full\":1}\n",
    "    X = 1  # chunk size\n",
    "    mask_height_span = MASK_SPAN.get('face', 1)\n",
    "    output_base_directory = \"output/carla_v1\"\n",
    "    input_video_base_path = f\"output/carla_v1/rgb/view\"\n",
    "    video_output_dir = \"output/carla_v1/videos/\"\n",
    "else:\n",
    "    num_views = 8\n",
    "    num_frames = 50\n",
    "    dataset_idx = 0#1\n",
    "    MASK_SPAN = {\"face\":1, \"no_feet\":1, \"full\":1}\n",
    "    X = 1  # chunk size\n",
    "    mask_height_span = MASK_SPAN.get('no_feet', 1)\n",
    "    # output_base_directory = \"jc_8_long_n2\"\n",
    "    output_base_directory = \"output/xr_lubna\"\n",
    "    input_video_base_path = f\"output/xr_lubna/rgb/view\"\n",
    "    video_output_dir = \"output/xr_lubna/videos/\"\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frame_range=range(50)):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        valid_views = [f\"view_{i}\" for i in range(num_views)]\n",
    "        valid_frames = frame_range  # frames 0 to 49\n",
    "        valid_image_names = [f\"pointcloud-{i}.png\" for i in valid_frames]\n",
    "        valid_depth_names = [f\"depth-{i}.png\" for i in valid_frames]\n",
    "\n",
    "\n",
    "        # We'll store data per scenario like:\n",
    "        # self.scenarios = [\n",
    "        #   {\n",
    "        #       \"images\": [[img_view0_frame0, ...], [img_view1_frame0, ...], ...], # [8][50]\n",
    "        #       \"gts\":    [[gt_view0_frame0, ...], ...],\n",
    "        #       \"depths\": [[depth_view0_frame0, ...], ...],\n",
    "        #       \"paths\":  [[path_view0_frame0, ...], ...]\n",
    "        #   }, ...\n",
    "        # ]\n",
    "        self.scenarios = []\n",
    "\n",
    "        # Identify scenario folders (excluding ground truth)\n",
    "        all_folders = [f for f in os.listdir(root_dir) \n",
    "                       if os.path.isdir(os.path.join(root_dir, f)) and \n",
    "                       not f.endswith('ground_truth') and '_ground_truth' not in f]\n",
    "        \n",
    "        \n",
    "\n",
    "        for folder in all_folders:\n",
    "            gt_folder = f\"{folder}_ground_truth\"\n",
    "            if not os.path.exists(os.path.join(root_dir, gt_folder)):\n",
    "                continue  # skip if no corresponding ground truth folder\n",
    "\n",
    "\n",
    "\n",
    "            scenario_images = []\n",
    "            scenario_gts = []\n",
    "            scenario_depths = []\n",
    "            scenario_paths = []\n",
    "\n",
    "            # For each view\n",
    "            for view_folder in valid_views:\n",
    "                image_folder_path = os.path.join(root_dir, folder, view_folder)\n",
    "                gt_folder_path = os.path.join(root_dir, gt_folder, view_folder)\n",
    "\n",
    "\n",
    "                image_folder_path += '/'\n",
    "\n",
    "                if not (os.path.exists(image_folder_path) and os.path.exists(gt_folder_path)):\n",
    "                    print('missing directories')\n",
    "                    scenario_images = []\n",
    "                    break\n",
    "\n",
    "\n",
    "                view_images = []\n",
    "                view_gts = []\n",
    "                view_depths = []\n",
    "                view_paths = []\n",
    "\n",
    "                # Make sure frames are in order\n",
    "                for frame_idx in valid_frames:\n",
    "                    image_name = f\"pointcloud-{frame_idx}.png\"\n",
    "                    depth_name = f\"depth-{frame_idx}.png\"\n",
    "                    image_path = os.path.join(image_folder_path, image_name)\n",
    "                    depth_path = os.path.join(image_folder_path, depth_name)\n",
    "                    gt_image_path = os.path.join(gt_folder_path, image_name)\n",
    "\n",
    "                    \n",
    "                    if not (os.path.exists(image_path) and os.path.exists(gt_image_path) and os.path.exists(depth_path)):\n",
    "                        print('missing directory')\n",
    "                        view_images = []\n",
    "                        break\n",
    "\n",
    "                    view_images.append(image_path)\n",
    "                    view_gts.append(gt_image_path)\n",
    "                    view_depths.append(depth_path)\n",
    "\n",
    "                    relative_path = image_path.replace(self.root_dir + \"/\", \"\")\n",
    "                    view_paths.append(relative_path)\n",
    "\n",
    "                # If any frame missing, break\n",
    "                print(len(view_images), num_frames)\n",
    "                if len(view_images) != num_frames:\n",
    "                    print('missing frame')\n",
    "                    scenario_images = []\n",
    "                    break\n",
    "\n",
    "                scenario_images.append(view_images)\n",
    "                scenario_gts.append(view_gts)\n",
    "                scenario_depths.append(view_depths)\n",
    "                scenario_paths.append(view_paths)\n",
    "\n",
    "\n",
    "            # Add the scenario if all views and frames loaded\n",
    "            if len(scenario_images) == num_views and all(len(v) == num_frames for v in scenario_images):\n",
    "                print(\"loaded scenario\")\n",
    "                self.scenarios.append({\n",
    "                    \"images\": scenario_images,\n",
    "                    \"gts\": scenario_gts,\n",
    "                    \"depths\": scenario_depths,\n",
    "                    \"paths\": scenario_paths\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        print(f\"Total scenarios loaded: {len(self.scenarios)}\")\n",
    "        return len(self.scenarios)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scenario = self.scenarios[idx]\n",
    "        scenario_images = scenario[\"images\"]   # [8][50]\n",
    "        scenario_gts = scenario[\"gts\"]         # [8][50]\n",
    "        scenario_depths = scenario[\"depths\"]   # [8][50]\n",
    "        scenario_paths = scenario[\"paths\"]     # [8][50]\n",
    "\n",
    "        # We'll load and process all images and masks\n",
    "        all_images = []  # Will hold [8, 50, 4, H, W] eventually\n",
    "        all_masks = []   # Will hold [8, 50, H, W]\n",
    "\n",
    "        # Define class colors\n",
    "        class_1_color = np.array([80, 239, 7])   # #50EF07\n",
    "        class_2_color = np.array([249, 0, 0])    # #F90000\n",
    "        tolerance = 30\n",
    "\n",
    "        for v in range(num_views):\n",
    "            view_imgs = []\n",
    "            view_masks = []\n",
    "            # For each frame in this view\n",
    "            for f in range(num_frames):\n",
    "                image_path = scenario_images[v][f]\n",
    "                gt_path = scenario_gts[v][f]\n",
    "                depth_path = scenario_depths[v][f]\n",
    "\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                gt_image = Image.open(gt_path).convert(\"RGB\")\n",
    "                depth_image = Image.open(depth_path)\n",
    "\n",
    "                gt_image_np = np.array(gt_image)\n",
    "                # Create mask\n",
    "                mask = np.zeros(gt_image_np.shape[:2], dtype=np.uint8)\n",
    "                mask[np.all(np.abs(gt_image_np - class_1_color) <= tolerance, axis=-1)] = 1\n",
    "                mask[np.all(np.abs(gt_image_np - class_2_color) <= tolerance, axis=-1)] = 2\n",
    "\n",
    "                # Convert images to tensors and apply transform if provided\n",
    "                if self.transform:\n",
    "                    # Apply transform to RGB image\n",
    "                    rgb_tensor = self.transform(image)  # [C,H,W]\n",
    "                    # Resize mask using nearest neighbor\n",
    "                    mask_pil = Image.fromarray(mask)\n",
    "                    mask_pil = mask_pil.resize((rgb_tensor.shape[2], rgb_tensor.shape[1]), Image.NEAREST)\n",
    "                    mask = np.array(mask_pil)\n",
    "\n",
    "                    # Resize depth image separately\n",
    "                    depth_resized = depth_image.resize((rgb_tensor.shape[2], rgb_tensor.shape[1]), Image.NEAREST)\n",
    "                    depth_np = np.array(depth_resized).astype(np.float32)\n",
    "\n",
    "                else:\n",
    "                    # If no transform, just convert directly\n",
    "                    rgb_tensor = transforms.ToTensor()(image)\n",
    "                    depth_np = np.array(depth_image).astype(np.float32)\n",
    "\n",
    "                # Normalize depth\n",
    "                # if depth_np.max() > 10 * depth_np.min():\n",
    "                #     depth_norm = depth_np/1000\n",
    "                # else:\n",
    "                #     depth_norm = depth_np  # all pixels same, no normalization needed\n",
    "                # depth_norm = depth_np/1000\n",
    "                depth_norm = depth_np\n",
    "\n",
    "                \n",
    "                depth_tensor = torch.tensor(depth_norm).unsqueeze(0)  # [1,H,W]\n",
    "\n",
    "                # Combine RGB and Depth into single tensor: [4,H,W]\n",
    "                img_with_depth = torch.cat((rgb_tensor, depth_tensor), dim=0)\n",
    "\n",
    "                # Convert mask to tensor\n",
    "                mask = torch.from_numpy(mask).long()\n",
    "\n",
    "                view_imgs.append(img_with_depth)  # [4,H,W]\n",
    "                view_masks.append(mask)           # [H,W]\n",
    "\n",
    "            # Stack frames for this view\n",
    "            view_imgs = torch.stack(view_imgs, dim=0)   # [50,4,H,W]\n",
    "            view_masks = torch.stack(view_masks, dim=0) # [50,H,W]\n",
    "\n",
    "            all_images.append(view_imgs)\n",
    "            all_masks.append(view_masks)\n",
    "\n",
    "        # Stack all views\n",
    "        all_images = torch.stack(all_images, dim=0)  # [8,50,4,H,W]\n",
    "        all_masks = torch.stack(all_masks, dim=0)    # [8,50,H,W]\n",
    "\n",
    "        return all_images, all_masks, scenario_paths\n",
    "\n",
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d38fb3c-ae58-41a6-a358-dcdbadca1dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jetsonuser/masking/datasets\n",
      "179 179\n",
      "loaded scenario\n",
      "Total scenarios loaded: 1\n",
      "Total scenarios loaded: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(dataset_dir)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# carla_framerange = range(68, 254)\n",
    "carla_framerange = range(179)\n",
    "\n",
    "dataset = SegmentationDataset(root_dir=dataset_dir, transform=transform, frame_range=carla_framerange)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dea16a-0bb8-4180-8cee-e9b1b1fc9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, paths = dataset[dataset_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbd303-ff27-4d09-ac3b-8f8589ffaf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carla_v1/view_0/pointcloud-0.png', 'carla_v1/view_0/pointcloud-1.png', 'carla_v1/view_0/pointcloud-2.png', 'carla_v1/view_0/pointcloud-3.png', 'carla_v1/view_0/pointcloud-4.png', 'carla_v1/view_0/pointcloud-5.png', 'carla_v1/view_0/pointcloud-6.png', 'carla_v1/view_0/pointcloud-7.png', 'carla_v1/view_0/pointcloud-8.png', 'carla_v1/view_0/pointcloud-9.png']\n"
     ]
    }
   ],
   "source": [
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9963f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── RetinaFace Setup (replace Faster R-CNN) ───────────────────────────────\n",
    "import torch, cv2\n",
    "\n",
    "# how often to re-run the detector vs. track\n",
    "DETECT_EVERY = 1\n",
    "\n",
    "# one MultiTracker per view\n",
    "trackers = [cv2.legacy.MultiTracker_create() for _ in range(num_views)]\n",
    "\n",
    "\n",
    "# ── Replacement detect_objects using Ultra-Light ──────────────────────────────\n",
    "DETECTION_CONFIDENCE_THRESHOLD = 0.3\n",
    "PRIVATE_OBJECT_CLASSES = ['person']  # used downstream by segment_all\n",
    "WINDOW_SIZE = 10\n",
    "DEPTH_THRESHOLD_MULTIPLIER = 75\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# from insightface.app import FaceAnalysis\n",
    "from retinaface import RetinaFace\n",
    "\n",
    "# # initialize once at top-level\n",
    "# retina_app = FaceAnalysis(allowed_modules=['detection'])\n",
    "# # ctx_id=0 for GPU, -1 for CPU\n",
    "# retina_app.prepare(ctx_id=0 if torch.cuda.is_available() else -1,\n",
    "#                    det_size=(2048,2048))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_objects(model, image_np, confidence_threshold=0.5, draw_boxes=False):\n",
    "    \"\"\"\n",
    "    image_np: BGR uint8 H×W×3\n",
    "    Returns list of {'box':[x1,y1,x2,y2],'score':…,'label':'person'}\n",
    "    or, if draw_boxes=True, returns the BGR image with boxes overlaid.\n",
    "    \"\"\"\n",
    "    # 1) run detection\n",
    "    faces = RetinaFace.detect_faces(image_np)\n",
    "    objects = []\n",
    "    print(faces)\n",
    "    for f, _ in faces.items():\n",
    "        score = faces[f]['score']\n",
    "        if score < confidence_threshold:\n",
    "            continue\n",
    "        x1,y1,x2,y2 = map(int, faces[f]['facial_area'])\n",
    "        objects.append({\"box\":[x1,y1,x2,y2],\n",
    "                        \"score\":float(score),\n",
    "                        \"label\":\"person\"})\n",
    "\n",
    "    if not draw_boxes:\n",
    "        return objects\n",
    "\n",
    "    # 2) draw\n",
    "    out = image_np.copy()\n",
    "    for obj in objects:\n",
    "        x1,y1,x2,y2 = obj[\"box\"]\n",
    "        s = obj[\"score\"]\n",
    "        cv2.rectangle(out, (x1,y1),(x2,y2), (255,0,0), 2)\n",
    "        cv2.putText(out, f\"{s:.2f}\", (x1, max(y1-5,0)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "    return out\n",
    "\n",
    "###############################################################################\n",
    "# 2) DEPTH PROFILE\n",
    "###############################################################################\n",
    "def calculate_depth_profile_of_box(depth_map, x1, y1, x2, y2, window_size=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Return { 'mean','std','threshold','box':[x1,y1,x2,y2] } or None if empty.\n",
    "    \"\"\"\n",
    "    half_window = window_size // 2\n",
    "    cx = (x1 + x2) // 2\n",
    "    cy = (y1 + y2) // 2\n",
    "\n",
    "    x_start = max(cx - half_window, 0)\n",
    "    x_end = min(cx + half_window + 1, depth_map.shape[1])\n",
    "    y_start = max(cy - half_window, 0)\n",
    "    y_end = min(cy + half_window + 1, depth_map.shape[0])\n",
    "\n",
    "    depth_window = depth_map[y_start:y_end, x_start:x_end]\n",
    "    depth_values = depth_window.flatten()\n",
    "    depth_values = depth_values[~np.isnan(depth_values)]\n",
    "    if depth_values.size == 0:\n",
    "        return None\n",
    "\n",
    "    depth_mean = float(np.mean(depth_values))\n",
    "    depth_std = float(np.std(depth_values))\n",
    "    depth_threshold = float(depth_std * DEPTH_THRESHOLD_MULTIPLIER)\n",
    "\n",
    "    return {\n",
    "        'mean': depth_mean,\n",
    "        'std': depth_std,\n",
    "        'threshold': depth_threshold,\n",
    "        'box': [x1,y1,x2,y2]\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 3) SEGMENT CHUNK\n",
    "###############################################################################\n",
    "\n",
    "def segment_person_from_box(depth_tensor, dprof, span=1):\n",
    "    \"\"\"\n",
    "    Similar to segment_person_from_profile_batch, but for a single bounding box\n",
    "    in just 1 frame's depth or multiple frames (X frames).\n",
    "    If depth_tensor: shape [X,H,W] or [H,W].\n",
    "    \"\"\"\n",
    "    if len(depth_tensor.shape) == 2:\n",
    "        # single frame => shape [H,W]\n",
    "        depth_tensor = depth_tensor.unsqueeze(0)  # => [1,H,W]\n",
    "\n",
    "    depth_mean = torch.tensor(dprof['mean'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    depth_thr  = torch.tensor(dprof['threshold'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    (x1,y1,x2,y2) = dprof['box']\n",
    "    y2 = int(y1 + span * (y2 - y1))\n",
    "\n",
    "\n",
    "    depth_diff = torch.abs(depth_tensor - depth_mean)\n",
    "    mask_batch = (depth_diff <= depth_thr).to(torch.uint8)\n",
    "\n",
    "    final_mask = torch.zeros_like(mask_batch)\n",
    "    _, H, W = depth_tensor.shape\n",
    "    x1_clamp = max(0, min(x1,W))\n",
    "    x2_clamp = max(0, min(x2,W))\n",
    "    y1_clamp = max(0, min(y1,H))\n",
    "    y2_clamp = max(0, min(y2,H))\n",
    "\n",
    "    if x2_clamp> x1_clamp and y2_clamp> y1_clamp:\n",
    "        final_mask[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp] = \\\n",
    "            mask_batch[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp]\n",
    "\n",
    "    # return shape [H,W] if single frame\n",
    "    if final_mask.shape[0] == 1:\n",
    "        return final_mask[0]\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "def segment_all(depth_tensor, objects, depth_map, span):\n",
    "    \"\"\"\n",
    "    We create a combined mask of shape [H,W] = 1 for each person's bounding box,\n",
    "    EXCEPT we skip the public_box (which is the \"public\" person).\n",
    "    depth_tensor: shape [H,W], float on GPU\n",
    "    objects: detection results on CPU\n",
    "    public_box: (x1,y1,x2,y2) that we skip\n",
    "    depth_map: CPU 2D array for depth\n",
    "    Return: torch.uint8 mask [H,W], 1=private, 0=public\n",
    "    \"\"\"\n",
    "    H, W = depth_tensor.shape[-2], depth_tensor.shape[-1]\n",
    "\n",
    "    combined_mask = torch.zeros((H,W), dtype=torch.uint8, device=depth_tensor.device)\n",
    "\n",
    "    for obj in objects:\n",
    "        if obj['label'] not in PRIVATE_OBJECT_CLASSES:\n",
    "            continue\n",
    "        box = obj['box']  # [x1,y1,x2,y2]\n",
    "\n",
    "        # otherwise, segment this bounding box => \"private\"\n",
    "        dprof = calculate_depth_profile_of_box(depth_map, *box)\n",
    "        if dprof is None:\n",
    "            continue\n",
    "\n",
    "        single_mask = segment_person_from_box(depth_tensor, dprof, span)\n",
    "        combined_mask = torch.logical_or(combined_mask.bool(), single_mask.bool()).to(torch.uint8)\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "###############################################################################\n",
    "# 4) METRICS\n",
    "###############################################################################\n",
    "def dice_score_batch(pred_batch, gt_batch):\n",
    "    intersection = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    pred_sum = torch.sum(pred_batch==1).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if (pred_sum+gt_sum)==0:\n",
    "        return 1.0\n",
    "    return 2.0*intersection/(pred_sum+gt_sum)\n",
    "\n",
    "def recall_batch(pred_batch, gt_batch):\n",
    "    tp = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if gt_sum==0:\n",
    "        return 1.0\n",
    "    return tp/gt_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b8a22-0254-4d90-935d-9548110b6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── RetinaFace Setup (replace Faster R-CNN) ───────────────────────────────\n",
    "import torch, cv2\n",
    "import sys\n",
    "sys.path.append('/home/jetsonuser/masking/Pytorch_Retinaface')\n",
    "\n",
    "# how often to re-run the detector vs. track\n",
    "DETECT_EVERY = 1\n",
    "\n",
    "# one MultiTracker per view\n",
    "# trackers = [cv2.MultiTracker_create() for _ in range(num_views)]\n",
    "\n",
    "\n",
    "# ── Replacement detect_objects using Ultra-Light ──────────────────────────────\n",
    "DETECTION_CONFIDENCE_THRESHOLD = 0.3\n",
    "PRIVATE_OBJECT_CLASSES = ['person']  # used downstream by segment_all\n",
    "WINDOW_SIZE = 10\n",
    "DEPTH_THRESHOLD_MULTIPLIER = 75\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# from insightface.app import FaceAnalysis\n",
    "from models.retinaface import RetinaFace\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.box_utils import decode\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "from data.config import cfg_mnet, cfg_re50\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Constants / globals\n",
    "DETECT_EVERY = 1\n",
    "DETECTION_CONFIDENCE_THRESHOLD = 0.8\n",
    "PRIVATE_OBJECT_CLASSES = ['person']\n",
    "WINDOW_SIZE = 10\n",
    "DEPTH_THRESHOLD_MULTIPLIER = 75\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model (already loaded earlier) - ensure it's on device and eval\n",
    "model = RetinaFace(cfg=cfg_re50, phase='test')\n",
    "# load checkpoint (you already have this logic; keep using strict=False if needed)\n",
    "state = torch.load('Pytorch_Retinaface/weights/Resnet50_Final.pth', weights_only=True, map_location=device)\n",
    "state_dict = state.get('state_dict', state) if isinstance(state, dict) else state\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Detection helper (fixed)\n",
    "def detect_objects(image_bgr, model, confidence_threshold, device):\n",
    "    \"\"\"\n",
    "    image_bgr: HxWx3 uint8 OpenCV BGR\n",
    "    Returns: numpy array of shape (N,5): [x1,y1,x2,y2,score]\n",
    "    \"\"\"\n",
    "    img = image_bgr.astype(np.float32)\n",
    "    img -= (104.0, 117.0, 123.0)\n",
    "    img = img.transpose(2, 0, 1)  # C,H,W\n",
    "    img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)  # [1,3,H,W]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loc, conf, landms = model(img_tensor)\n",
    "\n",
    "    # Build priorbox / decode with the same config used to instantiate the model\n",
    "    priorbox = PriorBox(cfg_re50, image_size=(img_tensor.shape[2], img_tensor.shape[3]))\n",
    "    priors = priorbox.forward().to(device)\n",
    "    boxes = decode(loc[0], priors, cfg_re50['variance'])  # [N,4], normalized\n",
    "    # scale to absolute image coords\n",
    "    _, _, H, W = img_tensor.shape\n",
    "    scale = torch.tensor([W, H, W, H], device=device)\n",
    "    boxes = boxes * scale  # now in pixels\n",
    "\n",
    "    scores = conf[0][:, 1]  # person class\n",
    "\n",
    "    # Filter by score\n",
    "    keep_mask = scores > confidence_threshold\n",
    "    if not keep_mask.any():\n",
    "        return np.zeros((0, 5), dtype=np.float32)\n",
    "\n",
    "    boxes = boxes[keep_mask]\n",
    "    scores = scores[keep_mask]\n",
    "    dets = torch.cat([boxes, scores.unsqueeze(1)], dim=1)  # [M,5]\n",
    "\n",
    "    dets = dets.cpu().numpy().astype(np.float32)\n",
    "    keep = py_cpu_nms(dets, 0.4)\n",
    "    dets = dets[keep]\n",
    "\n",
    "    # Drop degenerate / too-small boxes (width or height < 2 pixels)\n",
    "    filtered = []\n",
    "    for x1, y1, x2, y2, score in dets:\n",
    "        if (x2 - x1) < 2 or (y2 - y1) < 2:\n",
    "            continue\n",
    "        filtered.append([x1, y1, x2, y2, score])\n",
    "    if not filtered:\n",
    "        return np.zeros((0, 5), dtype=np.float32)\n",
    "    return np.array(filtered, dtype=np.float32)\n",
    "###############################################################################\n",
    "# 2) DEPTH PROFILE\n",
    "###############################################################################\n",
    "def calculate_depth_profile_of_box(depth_map, x1, y1, x2, y2, window_size=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Return { 'mean','std','threshold','box':[x1,y1,x2,y2] } or None if empty.\n",
    "    \"\"\"\n",
    "    half_window = window_size // 2\n",
    "    cx = (x1 + x2) // 2\n",
    "    cy = (y1 + y2) // 2\n",
    "\n",
    "    x_start = int(max(cx - half_window, 0))\n",
    "    x_end = int(min(cx + half_window + 1, depth_map.shape[1]))\n",
    "    y_start = int(max(cy - half_window, 0))\n",
    "    y_end = int(min(cy + half_window + 1, depth_map.shape[0]))\n",
    "\n",
    "    depth_window = depth_map[y_start:y_end, x_start:x_end]\n",
    "    depth_values = depth_window.flatten()\n",
    "    depth_values = depth_values[~np.isnan(depth_values)]\n",
    "    if depth_values.size == 0:\n",
    "        return None\n",
    "\n",
    "    depth_mean = float(np.mean(depth_values))\n",
    "    depth_std = float(np.std(depth_values))\n",
    "    depth_threshold = float(depth_std * DEPTH_THRESHOLD_MULTIPLIER)\n",
    "\n",
    "    return {\n",
    "        'mean': depth_mean,\n",
    "        'std': depth_std,\n",
    "        'threshold': depth_threshold,\n",
    "        'box': [x1,y1,x2,y2]\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 3) SEGMENT CHUNK\n",
    "###############################################################################\n",
    "\n",
    "def segment_person_from_box(depth_tensor, dprof, span=1):\n",
    "    \"\"\"\n",
    "    Similar to segment_person_from_profile_batch, but for a single bounding box\n",
    "    in just 1 frame's depth or multiple frames (X frames).\n",
    "    If depth_tensor: shape [X,H,W] or [H,W].\n",
    "    \"\"\"\n",
    "    if len(depth_tensor.shape) == 2:\n",
    "        # single frame => shape [H,W]\n",
    "        depth_tensor = depth_tensor.unsqueeze(0)  # => [1,H,W]\n",
    "\n",
    "    depth_mean = torch.tensor(dprof['mean'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    depth_thr  = torch.tensor(dprof['threshold'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    (x1,y1,x2,y2) = dprof['box']\n",
    "    y2 = int(y1 + span * (y2 - y1))\n",
    "\n",
    "\n",
    "    depth_diff = torch.abs(depth_tensor - depth_mean)\n",
    "    mask_batch = (depth_diff <= depth_thr).to(torch.uint8)\n",
    "\n",
    "    final_mask = torch.zeros_like(mask_batch)\n",
    "    _, H, W = depth_tensor.shape\n",
    "    x1_clamp = max(0, min(x1,W))\n",
    "    x2_clamp = max(0, min(x2,W))\n",
    "    y1_clamp = max(0, min(y1,H))\n",
    "    y2_clamp = max(0, min(y2,H))\n",
    "\n",
    "    if x2_clamp> x1_clamp and y2_clamp> y1_clamp:\n",
    "        final_mask[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp] = \\\n",
    "            mask_batch[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp]\n",
    "\n",
    "    # return shape [H,W] if single frame\n",
    "    if final_mask.shape[0] == 1:\n",
    "        return final_mask[0]\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "def segment_all(depth_tensor, objects, depth_map, span):\n",
    "    \"\"\"\n",
    "    We create a combined mask of shape [H,W] = 1 for each person's bounding box,\n",
    "    EXCEPT we skip the public_box (which is the \"public\" person).\n",
    "    depth_tensor: shape [H,W], float on GPU\n",
    "    objects: detection results on CPU\n",
    "    public_box: (x1,y1,x2,y2) that we skip\n",
    "    depth_map: CPU 2D array for depth\n",
    "    Return: torch.uint8 mask [H,W], 1=private, 0=public\n",
    "    \"\"\"\n",
    "    H, W = depth_tensor.shape[-2], depth_tensor.shape[-1]\n",
    "\n",
    "    combined_mask = torch.zeros((H,W), dtype=torch.uint8, device=depth_tensor.device)\n",
    "\n",
    "    for obj in objects:\n",
    "        if obj['label'] not in PRIVATE_OBJECT_CLASSES:\n",
    "            continue\n",
    "        box = obj['box']  # [x1,y1,x2,y2]\n",
    "\n",
    "        # otherwise, segment this bounding box => \"private\"\n",
    "        dprof = calculate_depth_profile_of_box(depth_map, *box)\n",
    "        if dprof is None:\n",
    "            continue\n",
    "\n",
    "        single_mask = segment_person_from_box(depth_tensor, dprof, span)\n",
    "        combined_mask = torch.logical_or(combined_mask.bool(), single_mask.bool()).to(torch.uint8)\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "###############################################################################\n",
    "# 4) METRICS\n",
    "###############################################################################\n",
    "def dice_score_batch(pred_batch, gt_batch):\n",
    "    intersection = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    pred_sum = torch.sum(pred_batch==1).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if (pred_sum+gt_sum)==0:\n",
    "        return 1.0\n",
    "    return 2.0*intersection/(pred_sum+gt_sum)\n",
    "\n",
    "def recall_batch(pred_batch, gt_batch):\n",
    "    tp = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if gt_sum==0:\n",
    "        return 1.0\n",
    "    return tp/gt_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3452cd-4e47-49cc-b77d-d2f7af4945d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk starts: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_f \u001b[38;5;241m%\u001b[39m DETECT_EVERY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# clear & re-seed the tracker\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     trackers[v] \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mMultiTracker_create()\n\u001b[0;32m---> 32\u001b[0m     dets \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbgr_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDETECTION_CONFIDENCE_THRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m dets:\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mdetect_objects\u001b[0;34m(image_bgr, model, confidence_threshold, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [1,3,H,W]\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 62\u001b[0m     loc, conf, landms \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Build priorbox / decode with the same config used to instantiate the model\u001b[39;00m\n\u001b[1;32m     65\u001b[0m priorbox \u001b[38;5;241m=\u001b[39m PriorBox(cfg_re50, image_size\u001b[38;5;241m=\u001b[39m(img_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], img_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Allocate mask tensor [V, F, H, W]\n",
    "pred_mask_full = torch.zeros(\n",
    "    (num_views, num_frames, images.shape[-2], images.shape[-1]),\n",
    "    dtype=torch.uint8,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "chunk_detection_times = []\n",
    "chunk_seg_times       = []\n",
    "chunk_total_times     = []\n",
    "\n",
    "chunk_starts = list(range(0, num_frames, X))\n",
    "print(f\"Chunk starts: {chunk_starts}\")\n",
    "\n",
    "for start_f in chunk_starts:\n",
    "    end_f = min(start_f + X, num_frames)\n",
    "    seg_start = time.time()\n",
    "\n",
    "    for v in range(num_views):\n",
    "        # 1) Prepare the BGR image for this view/frame\n",
    "        rgb_t = images[v, start_f, :3]                           # [3,H,W] float[0–1]\n",
    "        rgb_np = (rgb_t.permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "        bgr_np = cv2.cvtColor(rgb_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        det_start = time.perf_counter()\n",
    "        # 2) Decide: detect fresh or update tracker\n",
    "        if start_f % DETECT_EVERY == 0:\n",
    "            # clear & re-seed the tracker\n",
    "            trackers[v] = cv2.legacy.MultiTracker_create()\n",
    "            dets = detect_objects(bgr_np, None, DETECTION_CONFIDENCE_THRESHOLD, DEVICE)\n",
    "            boxes = []\n",
    "            for det in dets:\n",
    "                x1,y1,x2,y2 = det['box']\n",
    "                w, h = x2-x1, y2-y1\n",
    "                # add a new CSRT tracker for this box\n",
    "                trackers[v].add(cv2.legacy.TrackerCSRT_create(), bgr_np, (x1, y1, w, h))\n",
    "                boxes.append([x1,y1,x2,y2])\n",
    "        else:\n",
    "            # update existing trackers\n",
    "            ok, boxes_list = trackers[v].update(bgr_np)\n",
    "            # boxes_list is a tuple of (x,y,w,h)\n",
    "            boxes = [\n",
    "                [int(x), int(y), int(x+w), int(y+h)]\n",
    "                for (x,y,w,h) in boxes_list\n",
    "            ]\n",
    "        chunk_detection_times.append(time.perf_counter() - det_start)\n",
    "\n",
    "        # 3) Build a simple list for segmentation:\n",
    "        dets_for_seg = [{'box':b,'label':'person'} for b in boxes]\n",
    "\n",
    "        # 4) Depth‐segment unchanged\n",
    "        depth_map = images[v, start_f, 3].cpu().numpy()\n",
    "        depth_chunk = images[v, start_f:end_f, 3]  # [X,H,W]\n",
    "        private_mask = segment_all(depth_chunk, dets_for_seg, depth_map, mask_height_span)\n",
    "\n",
    "        # 5) Store your mask\n",
    "        pred_mask_full[v, start_f:end_f] = private_mask\n",
    "\n",
    "    seg_time = time.time() - seg_start\n",
    "    chunk_seg_times.append(seg_time)\n",
    "\n",
    "# Print out timing stats\n",
    "print(\"Detection times per chunk:\", chunk_detection_times)\n",
    "print(\"Segmentation times per chunk:\", chunk_seg_times)\n",
    "print(\"Total times per chunk:\", chunk_total_times)\n",
    "\n",
    "# (Optional) Compute metrics if using full-body masks\n",
    "# if mask_height_span > .8:\n",
    "#     dice = recall = 0\n",
    "#     for v in range(num_views):\n",
    "#         gt = (masks[v]==2).to(torch.uint8)\n",
    "#         d  = dice_score_batch(pred_mask_full[v].cpu(), gt.cpu())\n",
    "#         r  = recall_batch(pred_mask_full[v].cpu(), gt.cpu())\n",
    "#         dice += d; recall += r\n",
    "#         print(f\"View {v}: Dice={d:.4f}, Recall={r:.4f}\")\n",
    "#     print(\"Avg Dice:\", dice/num_views, \"Avg Recall:\", recall/num_views)\n",
    "\n",
    "# Overall timing\n",
    "num_chunks = len(chunk_starts)\n",
    "avg_det = sum(chunk_detection_times)/len(chunk_detection_times)\n",
    "avg_seg = sum(chunk_seg_times)/num_chunks\n",
    "avg_tot = sum(chunk_total_times)/num_chunks\n",
    "\n",
    "print(f\"Avg detection time     = {avg_det:.4f}s\")\n",
    "print(f\"Avg segmentation time  = {avg_seg:.4f}s\")\n",
    "print(f\"Avg total time/chunk   = {avg_tot:.4f}s\")\n",
    "print(f\"Total time all chunks  = {sum(chunk_total_times):.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eecbf03-3d50-4ac1-a290-344ad7c255c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(pred_mask_full[v].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8491da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def anonymize_region_gpu(\n",
    "    img_tensor: torch.Tensor,\n",
    "    mask_tensor: torch.Tensor,\n",
    "    method: str = \"fast_mosaic\",\n",
    "    block: int = 16,\n",
    "    noise_level: int = 25\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GPU-accelerated anonymization via PyTorch.\n",
    "\n",
    "    img_tensor: C×H×W tensor on GPU, dtype uint8 or float32[0-1]\n",
    "    mask_tensor: H×W boolean tensor on same device\n",
    "\n",
    "    Returns: anonymized C×H×W tensor on GPU, same dtype as input.\n",
    "    \"\"\"\n",
    "    # Prepare image in float [0,1]\n",
    "    was_uint8 = (img_tensor.dtype == torch.uint8)\n",
    "    if was_uint8:\n",
    "        img = img_tensor.to(torch.float32, device=img_tensor.device) / 255.0\n",
    "    else:\n",
    "        img = img_tensor\n",
    "\n",
    "    C, H, W = img.shape\n",
    "    out = img.clone()\n",
    "\n",
    "    # Find ROI bounding box\n",
    "    ys, xs = torch.where(mask_tensor)\n",
    "    if ys.numel() == 0:\n",
    "        # nothing to anonymize\n",
    "        return img_tensor\n",
    "    y1, y2 = int(ys.min().item()), int(ys.max().item()) + 1\n",
    "    x1, x2 = int(xs.min().item()), int(xs.max().item()) + 1\n",
    "\n",
    "    # Extract ROI\n",
    "    roi = out[:, y1:y2, x1:x2]  # C×h×w\n",
    "    mask_roi = mask_tensor[y1:y2, x1:x2]  # h×w\n",
    "\n",
    "    if method == \"fast_mosaic\":\n",
    "        # 1) downsample\n",
    "        small = F.interpolate(\n",
    "            roi.unsqueeze(0),\n",
    "            size=(max(1, (y2-y1)//block), max(1, (x2-x1)//block)),\n",
    "            mode=\"bilinear\", align_corners=False\n",
    "        ).squeeze(0)\n",
    "        # 2) upsample\n",
    "        mosaic = F.interpolate(\n",
    "            small.unsqueeze(0), (y2-y1, x2-x1), mode=\"nearest\"\n",
    "        ).squeeze(0)\n",
    "        # 3) add noise\n",
    "        noise = torch.randint(\n",
    "            -noise_level, noise_level+1,\n",
    "            mosaic.shape, device=img.device, dtype=torch.float32\n",
    "        ) / 255.0\n",
    "        noised = torch.clamp(mosaic + noise, 0.0, 1.0)\n",
    "        # 4) composite\n",
    "        mask_expand = mask_roi.unsqueeze(0).expand(C, -1, -1)\n",
    "        roi[mask_expand] = noised[mask_expand]\n",
    "        out[:, y1:y2, x1:x2] = roi\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    # Convert back to uint8 if needed\n",
    "    if was_uint8:\n",
    "        return (out * 255.0).to(torch.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def anonymize_depth(original_depth_np, noise_strength=0.01, output_path=None):\n",
    "\n",
    "    # --- Add Gaussian Noise (as before) ---\n",
    "    original_depth_np = original_depth_np.copy()  # Ensure we don't modify the original data\n",
    "    noise_mean_gaussian = 0.0\n",
    "    noise_std_dev_gaussian = noise_strength # Example: 5 millimeters\n",
    "    gaussian_noise = np.random.normal(noise_mean_gaussian, noise_std_dev_gaussian, original_depth_np.shape)\n",
    "    # Apply Gaussian noise to the original masked_depth\n",
    "    noisy_depth_gaussian = original_depth_np + gaussian_noise\n",
    "    # --- End Add Gaussian Noise ---\n",
    "\n",
    "    # --- Add Uniform Random Noise (Very Quick) ---\n",
    "    uniform_noise_magnitude = noise_strength/2\n",
    "\n",
    "    # Generate uniform noise within the range [-magnitude/2, +magnitude/2]\n",
    "    uniform_noise = np.random.uniform(\n",
    "        low=-uniform_noise_magnitude / 2.0,\n",
    "        high=uniform_noise_magnitude / 2.0,\n",
    "        size=noisy_depth_gaussian.shape\n",
    "    )\n",
    "\n",
    "    # Add the uniform noise to the already Gaussian-noisy depth\n",
    "    return noisy_depth_gaussian + uniform_noise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca63f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simple_lama_inpainting'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msimple_lama_inpainting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleLama\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simple_lama_inpainting'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from simple_lama_inpainting import SimpleLama\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm\n",
    "# from toonify_image import load_toonify_model, toonify_image_with_stylegan\n",
    "\n",
    "\n",
    "\n",
    "PSP_MODEL_PATH = \"pretrained_models/psp_toonify.pt\"\n",
    "\n",
    "\n",
    "model_used = True\n",
    "model_used = False\n",
    "\n",
    "if model_used:\n",
    "    simple_lama = SimpleLama()\n",
    "\n",
    "    # Load stable_diffusion from Hugging Face\n",
    "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-inpainting\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",  # necessary for speed\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # # Load SSD-1B from Hugging Face\n",
    "    # pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    #     \"\",\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     variant=\"fp16\",  # necessary for speed\n",
    "    # ).to(\"cuda\")\n",
    "\n",
    "    # load style GAN\n",
    "    loaded_toonify_model = load_toonify_model(PSP_MODEL_PATH)\n",
    "    print(\"Toonify model successfully loaded for the pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999ea37-5815-4083-b087-d0f4e59e4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_masked_images_gpu(pred_mask_full, images, out_folder, dilation_radius=4):\n",
    "    \"\"\"\n",
    "    Saves masked RGB & depth frames and records timing.\n",
    "    Returns dict with lists: chunk_total, chunk_anon, chunk_write.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    V, F, H, W = pred_mask_full.shape\n",
    "\n",
    "    # CPU kernel for dilation\n",
    "    k = 2 * dilation_radius + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "\n",
    "    # Make view folders\n",
    "    for v in range(V):\n",
    "        os.makedirs(os.path.join(out_folder, \"rgb\",   f\"view{v}\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(out_folder, \"depth\", f\"view{v}\"), exist_ok=True)\n",
    "\n",
    "    chunk_total_times = []\n",
    "    chunk_anonym_times = []\n",
    "    chunk_write_times = []\n",
    "\n",
    "    # Process in chunks of X frames\n",
    "    for start in range(0, F, X):\n",
    "        t_chunk_start = time.perf_counter()\n",
    "        anon_times = []\n",
    "        write_times = []\n",
    "\n",
    "        end = min(start + X, F)\n",
    "        for v in range(V):\n",
    "            for f in range(start, end):\n",
    "                # -- 1) dilate mask on CPU (fast) --\n",
    "                mask_np = pred_mask_full[v, f].cpu().numpy().astype(np.uint8)\n",
    "                mask_cpu = cv2.dilate(mask_np, kernel).astype(bool)\n",
    "\n",
    "                # -- 2) grab depth for saving (so we don't time I/O here) --\n",
    "                depth = images[v, f, 3].cpu().numpy()\n",
    "\n",
    "                # -- 3) move tensors to GPU --\n",
    "                rgb_gpu  = images[v, f, :3]                      # [3,H,W]\n",
    "                mask_gpu = torch.from_numpy(mask_cpu).to(rgb_gpu.device)  # [H,W]\n",
    "\n",
    "                # -- 4) anonymize once per chunk --\n",
    "                if f == start:\n",
    "                    t_anon_start = time.perf_counter()\n",
    "                    anon_gpu = anonymize_region_gpu(\n",
    "                        rgb_gpu, mask_gpu,\n",
    "                        method=\"fast_mosaic\",\n",
    "                        block=max(1, W//16),\n",
    "                        noise_level=20\n",
    "                    )\n",
    "                    # bring back to CPU uint8 H×W×3\n",
    "                    anon_arr = (\n",
    "                        anon_gpu.permute(1,2,0)\n",
    "                                 .cpu()\n",
    "                                 .numpy()\n",
    "                    )\n",
    "                    if anon_arr.dtype != np.uint8:\n",
    "                        anon_arr = (anon_arr * 255).clip(0,255).astype(np.uint8)\n",
    "                    anon_times.append(time.perf_counter() - t_anon_start)\n",
    "\n",
    "                # -- 5) composite --\n",
    "                orig = rgb_gpu.permute(1,2,0).cpu().numpy()\n",
    "                if orig.dtype != np.uint8:\n",
    "                    orig = (orig * 255).clip(0,255).astype(np.uint8)\n",
    "                out_rgb = orig.copy()\n",
    "                out_rgb[mask_cpu] = anon_arr[mask_cpu]\n",
    "\n",
    "                # -- 6) write & time it --\n",
    "                t_write_start = time.perf_counter()\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(out_folder, \"rgb\", f\"view{v}\", f\"{f}_masked.png\"),\n",
    "                    cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR)\n",
    "                )\n",
    "                depth_u16 = np.clip(depth + np.random.normal(0,10,depth.shape),\n",
    "                                     0,20000).astype(np.uint16)\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(out_folder, \"depth\", f\"view{v}\", f\"{f}_depth.png\"),\n",
    "                    depth_u16\n",
    "                )\n",
    "                write_times.append(time.perf_counter() - t_write_start)\n",
    "\n",
    "        # record this chunk’s timings\n",
    "        chunk_total_times.append(time.perf_counter() - t_chunk_start)\n",
    "        chunk_anonym_times.append(sum(anon_times))\n",
    "        chunk_write_times.append(sum(write_times))\n",
    "\n",
    "    # return the timing lists for analysis\n",
    "    return {\n",
    "        \"chunk_total\": chunk_total_times,\n",
    "        \"chunk_anon\":  chunk_anonym_times,\n",
    "        \"chunk_write\": chunk_write_times\n",
    "    }\n",
    "\n",
    "# Usage:\n",
    "timings = save_masked_images_gpu(\n",
    "    pred_mask_full, images, output_base_directory, dilation_radius=4\n",
    ")\n",
    "print(\"Per-chunk totals:\", timings[\"chunk_total\"])\n",
    "print(\"Per-chunk anonym times:\", timings[\"chunk_anon\"])\n",
    "print(\"Per-chunk write times:\", timings[\"chunk_write\"])\n",
    "print(\"Avg total:\", np.mean(timings[\"chunk_total\"]))\n",
    "print(\"Avg anon:\",  np.mean(timings[\"chunk_anon\"]))\n",
    "print(\"Avg write:\", np.mean(timings[\"chunk_write\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masking image 0\n",
      "{'face_1': {'score': 0.998749315738678, 'facial_area': [1129, 488, 1169, 536], 'landmarks': {'right_eye': [1134.6956, 505.5952], 'left_eye': [1148.9933, 509.3922], 'nose': [1133.8142, 516.02136], 'mouth_right': [1133.5044, 524.1914], 'mouth_left': [1142.848, 526.9775]}}, 'face_2': {'score': 0.9778648018836975, 'facial_area': [829, 407, 843, 426], 'landmarks': {'right_eye': [835.72406, 415.71954], 'left_eye': [841.5414, 415.94122], 'nose': [839.27496, 419.1503], 'mouth_right': [835.76654, 421.97803], 'mouth_left': [840.411, 422.2338]}}, 'face_3': {'score': 0.964948296546936, 'facial_area': [635, 388, 643, 397], 'landmarks': {'right_eye': [637.9238, 391.67953], 'left_eye': [641.2111, 391.77588], 'nose': [639.5469, 393.5299], 'mouth_right': [638.10187, 395.31577], 'mouth_left': [640.762, 395.38675]}}}\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'imwrite'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m         det_img \u001b[38;5;241m=\u001b[39m detect_objects(\u001b[38;5;28;01mNone\u001b[39;00m, img,\n\u001b[1;32m     27\u001b[0m                                  confidence_threshold\u001b[38;5;241m=\u001b[39mDETECTION_CONFIDENCE_THRESHOLD,\n\u001b[1;32m     28\u001b[0m                                  )\n\u001b[1;32m     30\u001b[0m         out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(det_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_detect.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m         \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ All detection‐overlay images saved under:\u001b[39m\u001b[38;5;124m\"\u001b[39m, det_root)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'imwrite'\n> Overload resolution failed:\n>  - img is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'img'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Paths\n",
    "base = output_base_directory                  # e.g. \"output/xr_lubna\"\n",
    "mask_root = os.path.join(base, \"rgb\")         # your masked imgs: rgb/view0/0_masked.png...\n",
    "det_root  = os.path.join(base, \"rgb_detect\")\n",
    "\n",
    "os.makedirs(det_root, exist_ok=True)\n",
    "\n",
    "for v in range(num_views):\n",
    "    mask_dir = os.path.join(mask_root, f\"view{v}\")\n",
    "    det_dir  = os.path.join(det_root,  f\"view{v}\")\n",
    "    os.makedirs(det_dir, exist_ok=True)\n",
    "    \n",
    "    for f in range(num_frames):\n",
    "        mask_path = os.path.join(mask_dir, f\"{f}_masked.png\")\n",
    "        img = cv2.imread(mask_path)\n",
    "        if img is None:\n",
    "            print(f\"⚠️ Missing frame {mask_path}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Run your detector in BGR uint8\n",
    "        # (switch to detect_fast_scrfd if you prefer SCRFD)\n",
    "        det_img = detect_objects(None, img,\n",
    "                                 confidence_threshold=DETECTION_CONFIDENCE_THRESHOLD,\n",
    "                                 draw_boxes=True)\n",
    "\n",
    "        out_path = os.path.join(det_dir, f\"{f}_detect.png\")\n",
    "        cv2.imwrite(out_path, det_img)\n",
    "\n",
    "print(\"✅ All detection‐overlay images saved under:\", det_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _save_debug_views(\n",
    "    rgb_gpu, anon_gpu, mask_gpu, out_rgb_gpu,\n",
    "    out_folder, v, f, suffix=\"\",\n",
    "):\n",
    "    \"\"\"Helper to dump debug diagnostics for a frame.\"\"\"\n",
    "    # Ensure everything is in [0,1]\n",
    "    def to_uint8(tensor):\n",
    "        return (\n",
    "            tensor.clamp(0, 1)\n",
    "                  .mul(255)\n",
    "                  .to(torch.uint8)\n",
    "                  .permute(1, 2, 0)  # HWC\n",
    "                  .cpu()\n",
    "                  .numpy()\n",
    "        )\n",
    "\n",
    "    orig = to_uint8(rgb_gpu)\n",
    "    anon = to_uint8(anon_gpu)\n",
    "    comp = to_uint8(out_rgb_gpu)\n",
    "    mask_vis = (mask_gpu.cpu().numpy().astype(np.uint8) * 255)\n",
    "\n",
    "    view_dir = os.path.join(out_folder, \"debug\", f\"view{v}\")\n",
    "    os.makedirs(view_dir, exist_ok=True)\n",
    "\n",
    "    cv2.imwrite(os.path.join(view_dir, f\"{f:06d}_orig{suffix}.png\"), cv2.cvtColor(orig, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(os.path.join(view_dir, f\"{f:06d}_anon{suffix}.png\"), cv2.cvtColor(anon, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(os.path.join(view_dir, f\"{f:06d}_mask{suffix}.png\"), mask_vis)\n",
    "    cv2.imwrite(os.path.join(view_dir, f\"{f:06d}_comp{suffix}.png\"), cv2.cvtColor(comp, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "def save_masked_images_gpu(\n",
    "    pred_mask_full,\n",
    "    images,\n",
    "    out_folder,\n",
    "    X,\n",
    "    dilation_radius=4,\n",
    "    device=None,\n",
    "    debug_frames=2  # how many frames per view to dump debug for\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves masked RGB & depth frames with anonymization, with diagnostics and fallback.\n",
    "    pred_mask_full: [V, F, H, W] uint8\n",
    "    images: [V, F, C, H, W] with C>=4, RGB expected in [:3] in [0,1] floats (or [0,255], auto-normalized)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    V, F, H, W = pred_mask_full.shape\n",
    "\n",
    "    # CPU kernel for dilation\n",
    "    k = 2 * dilation_radius + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "\n",
    "    # Make view folders\n",
    "    for v in range(V):\n",
    "        os.makedirs(os.path.join(out_folder, \"rgb\", f\"view{v}\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(out_folder, \"depth\", f\"view{v}\"), exist_ok=True)\n",
    "\n",
    "    chunk_total_times = []\n",
    "    chunk_anonym_times = []\n",
    "    chunk_write_times = []\n",
    "\n",
    "    for start in range(0, F, X):\n",
    "        t_chunk_start = time.perf_counter()\n",
    "        anon_times = []\n",
    "        write_times = []\n",
    "\n",
    "        end = min(start + X, F)\n",
    "        for v in range(V):\n",
    "            for f in range(start, end):\n",
    "                # -- 1) dilate mask on CPU --\n",
    "                mask_tensor = pred_mask_full[v, f]  # uint8\n",
    "                mask_np = mask_tensor.cpu().numpy().astype(np.uint8)\n",
    "                dilated = cv2.dilate(mask_np, kernel)\n",
    "                mask_bool = dilated.astype(bool)\n",
    "                mask_gpu = torch.from_numpy(mask_bool).to(device)  # [H,W], bool-like\n",
    "\n",
    "                # -- 2) depth for saving --\n",
    "                depth = images[v, f, 3].cpu().numpy()\n",
    "\n",
    "                # -- 3) prepare RGB on GPU and normalize if needed --\n",
    "                rgb_gpu = images[v, f, :3].to(device)  # [3,H,W]\n",
    "                # Heuristic: if in 0-255 range, scale to 0-1\n",
    "                if rgb_gpu.max() > 1.5:\n",
    "                    rgb_gpu = rgb_gpu / 255.0\n",
    "\n",
    "                # -- 4) anonymize per frame --\n",
    "                t_anon_start = time.perf_counter()\n",
    "                anon_gpu = anonymize_region_gpu(\n",
    "                    rgb_gpu, mask_gpu,\n",
    "                    method=\"fast_mosaic\",\n",
    "                    block=max(1, W // 16),\n",
    "                    noise_level=20\n",
    "                )  # expect [3,H,W] in same range as rgb_gpu\n",
    "                anon_time = time.perf_counter() - t_anon_start\n",
    "                anon_times.append(anon_time)\n",
    "\n",
    "                # -- 5) fallback if anonymization looks degenerate (e.g., all zeros or very low variance) --\n",
    "                # Use simple mosaic if anon_gpu is near-zero or identical to input\n",
    "                with torch.no_grad():\n",
    "                    # Check signal strength\n",
    "                    if anon_gpu.abs().max() < 1e-3 or torch.allclose(anon_gpu, rgb_gpu, atol=1e-3):\n",
    "                        block = max(1, W // 16)\n",
    "                        small = F.avg_pool2d(rgb_gpu.unsqueeze(0), kernel_size=block, stride=block, padding=0)\n",
    "                        up = F.interpolate(small, size=(H, W), mode=\"nearest\").squeeze(0)\n",
    "                        anon_gpu = up  # override\n",
    "\n",
    "                # -- 6) composite on GPU --\n",
    "                mask_expand = mask_gpu.unsqueeze(0)  # [1,H,W]\n",
    "                out_rgb_gpu = torch.where(mask_expand, anon_gpu, rgb_gpu)\n",
    "\n",
    "                # Dump debug for first few frames per view\n",
    "                if f < debug_frames:\n",
    "                    _save_debug_views(\n",
    "                        rgb_gpu, anon_gpu, mask_gpu, out_rgb_gpu,\n",
    "                        out_folder, v, f\n",
    "                    )\n",
    "\n",
    "                # -- 7) write out --\n",
    "                t_write_start = time.perf_counter()\n",
    "                out_rgb = (\n",
    "                    out_rgb_gpu.clamp(0, 1)\n",
    "                               .mul(255)\n",
    "                               .to(torch.uint8)\n",
    "                               .permute(1, 2, 0)  # HWC\n",
    "                               .cpu()\n",
    "                               .numpy()\n",
    "                )\n",
    "                fname_rgb = f\"{f:06d}_masked.png\"\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(out_folder, \"rgb\", f\"view{v}\", fname_rgb),\n",
    "                    cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR)\n",
    "                )\n",
    "                # Depth with small visualization noise\n",
    "                depth_u16 = np.clip(depth + np.random.normal(0, 10, depth.shape), 0, 20000).astype(np.uint16)\n",
    "                fname_depth = f\"{f:06d}_depth.png\"\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(out_folder, \"depth\", f\"view{v}\", fname_depth),\n",
    "                    depth_u16\n",
    "                )\n",
    "                write_times.append(time.perf_counter() - t_write_start)\n",
    "\n",
    "        chunk_total_times.append(time.perf_counter() - t_chunk_start)\n",
    "        chunk_anonym_times.append(sum(anon_times))\n",
    "        chunk_write_times.append(sum(write_times))\n",
    "\n",
    "    return {\n",
    "        \"chunk_total\": chunk_total_times,\n",
    "        \"chunk_anon\": chunk_anonym_times,\n",
    "        \"chunk_write\": chunk_write_times\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55031995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated FPS: 24.24 frames per second\n"
     ]
    }
   ],
   "source": [
    "fps = 1/((avg_anon + avg_seg)/ X)\n",
    "print(f\"Estimated FPS: {fps:.2f} frames per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b117b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to output/carla_v1/videos/view0.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "fps=30\n",
    "# Set parameters\n",
    "num_frames = num_frames  # Already defined in your notebook\n",
    "\n",
    "os.makedirs(video_output_dir, exist_ok=True)\n",
    "for view_idx in range(num_views):\n",
    "    output_path = os.path.join(video_output_dir, f\"view{view_idx}.mp4\")\n",
    "    \n",
    "    input_video_pattern = input_video_base_path + f\"{view_idx}/{{}}_masked.png\"\n",
    "    # input_video_pattern = input_video_base_path + f\"{view_idx}/{{}}_detect.png\"\n",
    "\n",
    "    # Read the first frame to get the size\n",
    "    first_frame_path = input_video_pattern.format(0)\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    if first_frame is None:\n",
    "        raise FileNotFoundError(f\"First frame not found: {first_frame_path}\")\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    # Define the video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write frames to video\n",
    "    for i in range(num_frames):\n",
    "        frame_path = input_video_pattern.format(i)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Frame not found: {frame_path}, skipping.\")\n",
    "            continue\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f49bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb8ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2adde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetson-pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
