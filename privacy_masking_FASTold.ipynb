{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd50a6e0-338d-4fba-b4c5-7477bb0c56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/JiahuiYu/neuralgym\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CARLA_DATA = True\n",
    "\n",
    "# dataset_dir = \"/home/summer2025/AVPrivacy/carla_sim/test_images2/\"\n",
    "dataset_dir = \"/home/jetsonuser/masking/datasets\"\n",
    "\n",
    "if CARLA_DATA:\n",
    "    num_views = 1\n",
    "    num_frames = 80\n",
    "    # num_frames = 10\n",
    "    dataset_idx = 0\n",
    "    MASK_SPAN = {\"face\":1, \"no_feet\":1, \"full\":1}\n",
    "    X = 1  # chunk size\n",
    "    mask_height_span = MASK_SPAN.get('face', 1)\n",
    "    output_base_directory = \"output/carla_v1\"\n",
    "    input_video_base_path = f\"output/carla_v1/rgb/view\"\n",
    "    video_output_dir = \"output/carla_v1/videos/\"\n",
    "else:\n",
    "    num_views = 8\n",
    "    num_frames = 50\n",
    "    dataset_idx = 0#1\n",
    "    MASK_SPAN = {\"face\":1, \"no_feet\":1, \"full\":1}\n",
    "    X = 1  # chunk size\n",
    "    mask_height_span = MASK_SPAN.get('no_feet', 1)\n",
    "    # output_base_directory = \"jc_8_long_n2\"\n",
    "    output_base_directory = \"output/xr_lubna\"\n",
    "    input_video_base_path = f\"output/xr_lubna/rgb/view\"\n",
    "    video_output_dir = \"output/xr_lubna/videos/\"\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, frame_range=range(50)):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        valid_views = [f\"view_{i}\" for i in range(num_views)]\n",
    "        valid_frames = frame_range  # frames 0 to 49\n",
    "        valid_image_names = [f\"pointcloud-{i}.png\" for i in valid_frames]\n",
    "        valid_depth_names = [f\"depth-{i}.png\" for i in valid_frames]\n",
    "\n",
    "\n",
    "        # We'll store data per scenario like:\n",
    "        # self.scenarios = [\n",
    "        #   {\n",
    "        #       \"images\": [[img_view0_frame0, ...], [img_view1_frame0, ...], ...], # [8][50]\n",
    "        #       \"gts\":    [[gt_view0_frame0, ...], ...],\n",
    "        #       \"depths\": [[depth_view0_frame0, ...], ...],\n",
    "        #       \"paths\":  [[path_view0_frame0, ...], ...]\n",
    "        #   }, ...\n",
    "        # ]\n",
    "        self.scenarios = []\n",
    "\n",
    "        # Identify scenario folders (excluding ground truth)\n",
    "        all_folders = [f for f in os.listdir(root_dir) \n",
    "                       if os.path.isdir(os.path.join(root_dir, f)) and \n",
    "                       not f.endswith('ground_truth') and '_ground_truth' not in f]\n",
    "        \n",
    "        \n",
    "\n",
    "        for folder in all_folders:\n",
    "            gt_folder = f\"{folder}_ground_truth\"\n",
    "            if not os.path.exists(os.path.join(root_dir, gt_folder)):\n",
    "                continue  # skip if no corresponding ground truth folder\n",
    "\n",
    "\n",
    "\n",
    "            scenario_images = []\n",
    "            scenario_gts = []\n",
    "            scenario_depths = []\n",
    "            scenario_paths = []\n",
    "\n",
    "            # For each view\n",
    "            for view_folder in valid_views:\n",
    "                image_folder_path = os.path.join(root_dir, folder, view_folder)\n",
    "                gt_folder_path = os.path.join(root_dir, gt_folder, view_folder)\n",
    "\n",
    "\n",
    "                image_folder_path += '/'\n",
    "\n",
    "                if not (os.path.exists(image_folder_path) and os.path.exists(gt_folder_path)):\n",
    "                    print('missing directories')\n",
    "                    scenario_images = []\n",
    "                    break\n",
    "\n",
    "\n",
    "                view_images = []\n",
    "                view_gts = []\n",
    "                view_depths = []\n",
    "                view_paths = []\n",
    "\n",
    "                # Make sure frames are in order\n",
    "                for frame_idx in valid_frames:\n",
    "                    image_name = f\"pointcloud-{frame_idx}.png\"\n",
    "                    depth_name = f\"depth-{frame_idx}.png\"\n",
    "                    image_path = os.path.join(image_folder_path, image_name)\n",
    "                    depth_path = os.path.join(image_folder_path, depth_name)\n",
    "                    gt_image_path = os.path.join(gt_folder_path, image_name)\n",
    "\n",
    "                    \n",
    "                    if not (os.path.exists(image_path) and os.path.exists(gt_image_path) and os.path.exists(depth_path)):\n",
    "                        print('missing directory')\n",
    "                        view_images = []\n",
    "                        break\n",
    "\n",
    "                    view_images.append(image_path)\n",
    "                    view_gts.append(gt_image_path)\n",
    "                    view_depths.append(depth_path)\n",
    "\n",
    "                    relative_path = image_path.replace(self.root_dir + \"/\", \"\")\n",
    "                    view_paths.append(relative_path)\n",
    "\n",
    "                # If any frame missing, break\n",
    "                print(len(view_images), num_frames)\n",
    "                if len(view_images) != num_frames:\n",
    "                    print('missing frame')\n",
    "                    scenario_images = []\n",
    "                    break\n",
    "\n",
    "                scenario_images.append(view_images)\n",
    "                scenario_gts.append(view_gts)\n",
    "                scenario_depths.append(view_depths)\n",
    "                scenario_paths.append(view_paths)\n",
    "\n",
    "\n",
    "            # Add the scenario if all views and frames loaded\n",
    "            if len(scenario_images) == num_views and all(len(v) == num_frames for v in scenario_images):\n",
    "                print(\"loaded scenario\")\n",
    "                self.scenarios.append({\n",
    "                    \"images\": scenario_images,\n",
    "                    \"gts\": scenario_gts,\n",
    "                    \"depths\": scenario_depths,\n",
    "                    \"paths\": scenario_paths\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        print(f\"Total scenarios loaded: {len(self.scenarios)}\")\n",
    "        return len(self.scenarios)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scenario = self.scenarios[idx]\n",
    "        scenario_images = scenario[\"images\"]   # [8][50]\n",
    "        scenario_gts = scenario[\"gts\"]         # [8][50]\n",
    "        scenario_depths = scenario[\"depths\"]   # [8][50]\n",
    "        scenario_paths = scenario[\"paths\"]     # [8][50]\n",
    "\n",
    "        # We'll load and process all images and masks\n",
    "        all_images = []  # Will hold [8, 50, 4, H, W] eventually\n",
    "        all_masks = []   # Will hold [8, 50, H, W]\n",
    "\n",
    "        # Define class colors\n",
    "        class_1_color = np.array([80, 239, 7])   # #50EF07\n",
    "        class_2_color = np.array([249, 0, 0])    # #F90000\n",
    "        tolerance = 30\n",
    "\n",
    "        for v in range(num_views):\n",
    "            view_imgs = []\n",
    "            view_masks = []\n",
    "            # For each frame in this view\n",
    "            for f in range(num_frames):\n",
    "                image_path = scenario_images[v][f]\n",
    "                gt_path = scenario_gts[v][f]\n",
    "                depth_path = scenario_depths[v][f]\n",
    "\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                gt_image = Image.open(gt_path).convert(\"RGB\")\n",
    "                depth_image = Image.open(depth_path)\n",
    "\n",
    "                gt_image_np = np.array(gt_image)\n",
    "                # Create mask\n",
    "                mask = np.zeros(gt_image_np.shape[:2], dtype=np.uint8)\n",
    "                mask[np.all(np.abs(gt_image_np - class_1_color) <= tolerance, axis=-1)] = 1\n",
    "                mask[np.all(np.abs(gt_image_np - class_2_color) <= tolerance, axis=-1)] = 2\n",
    "\n",
    "                # Convert images to tensors and apply transform if provided\n",
    "                if self.transform:\n",
    "                    # Apply transform to RGB image\n",
    "                    rgb_tensor = self.transform(image)  # [C,H,W]\n",
    "                    # Resize mask using nearest neighbor\n",
    "                    mask_pil = Image.fromarray(mask)\n",
    "                    mask_pil = mask_pil.resize((rgb_tensor.shape[2], rgb_tensor.shape[1]), Image.NEAREST)\n",
    "                    mask = np.array(mask_pil)\n",
    "\n",
    "                    # Resize depth image separately\n",
    "                    depth_resized = depth_image.resize((rgb_tensor.shape[2], rgb_tensor.shape[1]), Image.NEAREST)\n",
    "                    depth_np = np.array(depth_resized).astype(np.float32)\n",
    "\n",
    "                else:\n",
    "                    # If no transform, just convert directly\n",
    "                    rgb_tensor = transforms.ToTensor()(image)\n",
    "                    depth_np = np.array(depth_image).astype(np.float32)\n",
    "\n",
    "                # Normalize depth\n",
    "                # if depth_np.max() > 10 * depth_np.min():\n",
    "                #     depth_norm = depth_np/1000\n",
    "                # else:\n",
    "                #     depth_norm = depth_np  # all pixels same, no normalization needed\n",
    "                # depth_norm = depth_np/1000\n",
    "                depth_norm = depth_np\n",
    "\n",
    "                \n",
    "                depth_tensor = torch.tensor(depth_norm).unsqueeze(0)  # [1,H,W]\n",
    "\n",
    "                # Combine RGB and Depth into single tensor: [4,H,W]\n",
    "                img_with_depth = torch.cat((rgb_tensor, depth_tensor), dim=0)\n",
    "\n",
    "                # Convert mask to tensor\n",
    "                mask = torch.from_numpy(mask).long()\n",
    "\n",
    "                view_imgs.append(img_with_depth)  # [4,H,W]\n",
    "                view_masks.append(mask)           # [H,W]\n",
    "\n",
    "            # Stack frames for this view\n",
    "            view_imgs = torch.stack(view_imgs, dim=0)   # [50,4,H,W]\n",
    "            view_masks = torch.stack(view_masks, dim=0) # [50,H,W]\n",
    "\n",
    "            all_images.append(view_imgs)\n",
    "            all_masks.append(view_masks)\n",
    "\n",
    "        # Stack all views\n",
    "        all_images = torch.stack(all_images, dim=0)  # [8,50,4,H,W]\n",
    "        all_masks = torch.stack(all_masks, dim=0)    # [8,50,H,W]\n",
    "\n",
    "        return all_images, all_masks, scenario_paths\n",
    "\n",
    "# Example usage\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d38fb3c-ae58-41a6-a358-dcdbadca1dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jetsonuser/masking/datasets\n",
      "80 80\n",
      "loaded scenario\n",
      "Total scenarios loaded: 1\n",
      "Total scenarios loaded: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(dataset_dir)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# carla_framerange = range(68, 254)\n",
    "carla_framerange = range(80)\n",
    "\n",
    "dataset = SegmentationDataset(root_dir=dataset_dir, transform=transform, frame_range=carla_framerange)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0dea16a-0bb8-4180-8cee-e9b1b1fc9542",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, masks, paths = dataset[dataset_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0dbd303-ff27-4d09-ac3b-8f8589ffaf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carla_v1/view_0/pointcloud-0.png', 'carla_v1/view_0/pointcloud-1.png', 'carla_v1/view_0/pointcloud-2.png', 'carla_v1/view_0/pointcloud-3.png', 'carla_v1/view_0/pointcloud-4.png', 'carla_v1/view_0/pointcloud-5.png', 'carla_v1/view_0/pointcloud-6.png', 'carla_v1/view_0/pointcloud-7.png', 'carla_v1/view_0/pointcloud-8.png', 'carla_v1/view_0/pointcloud-9.png', 'carla_v1/view_0/pointcloud-10.png', 'carla_v1/view_0/pointcloud-11.png', 'carla_v1/view_0/pointcloud-12.png', 'carla_v1/view_0/pointcloud-13.png', 'carla_v1/view_0/pointcloud-14.png', 'carla_v1/view_0/pointcloud-15.png', 'carla_v1/view_0/pointcloud-16.png', 'carla_v1/view_0/pointcloud-17.png', 'carla_v1/view_0/pointcloud-18.png', 'carla_v1/view_0/pointcloud-19.png', 'carla_v1/view_0/pointcloud-20.png', 'carla_v1/view_0/pointcloud-21.png', 'carla_v1/view_0/pointcloud-22.png', 'carla_v1/view_0/pointcloud-23.png', 'carla_v1/view_0/pointcloud-24.png', 'carla_v1/view_0/pointcloud-25.png', 'carla_v1/view_0/pointcloud-26.png', 'carla_v1/view_0/pointcloud-27.png', 'carla_v1/view_0/pointcloud-28.png', 'carla_v1/view_0/pointcloud-29.png', 'carla_v1/view_0/pointcloud-30.png', 'carla_v1/view_0/pointcloud-31.png', 'carla_v1/view_0/pointcloud-32.png', 'carla_v1/view_0/pointcloud-33.png', 'carla_v1/view_0/pointcloud-34.png', 'carla_v1/view_0/pointcloud-35.png', 'carla_v1/view_0/pointcloud-36.png', 'carla_v1/view_0/pointcloud-37.png', 'carla_v1/view_0/pointcloud-38.png', 'carla_v1/view_0/pointcloud-39.png', 'carla_v1/view_0/pointcloud-40.png', 'carla_v1/view_0/pointcloud-41.png', 'carla_v1/view_0/pointcloud-42.png', 'carla_v1/view_0/pointcloud-43.png', 'carla_v1/view_0/pointcloud-44.png', 'carla_v1/view_0/pointcloud-45.png', 'carla_v1/view_0/pointcloud-46.png', 'carla_v1/view_0/pointcloud-47.png', 'carla_v1/view_0/pointcloud-48.png', 'carla_v1/view_0/pointcloud-49.png', 'carla_v1/view_0/pointcloud-50.png', 'carla_v1/view_0/pointcloud-51.png', 'carla_v1/view_0/pointcloud-52.png', 'carla_v1/view_0/pointcloud-53.png', 'carla_v1/view_0/pointcloud-54.png', 'carla_v1/view_0/pointcloud-55.png', 'carla_v1/view_0/pointcloud-56.png', 'carla_v1/view_0/pointcloud-57.png', 'carla_v1/view_0/pointcloud-58.png', 'carla_v1/view_0/pointcloud-59.png', 'carla_v1/view_0/pointcloud-60.png', 'carla_v1/view_0/pointcloud-61.png', 'carla_v1/view_0/pointcloud-62.png', 'carla_v1/view_0/pointcloud-63.png', 'carla_v1/view_0/pointcloud-64.png', 'carla_v1/view_0/pointcloud-65.png', 'carla_v1/view_0/pointcloud-66.png', 'carla_v1/view_0/pointcloud-67.png', 'carla_v1/view_0/pointcloud-68.png', 'carla_v1/view_0/pointcloud-69.png', 'carla_v1/view_0/pointcloud-70.png', 'carla_v1/view_0/pointcloud-71.png', 'carla_v1/view_0/pointcloud-72.png', 'carla_v1/view_0/pointcloud-73.png', 'carla_v1/view_0/pointcloud-74.png', 'carla_v1/view_0/pointcloud-75.png', 'carla_v1/view_0/pointcloud-76.png', 'carla_v1/view_0/pointcloud-77.png', 'carla_v1/view_0/pointcloud-78.png', 'carla_v1/view_0/pointcloud-79.png']\n"
     ]
    }
   ],
   "source": [
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4b8a22-0254-4d90-935d-9548110b6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "model ignore: /home/jetsonuser/.insightface/models/buffalo_s/1k3d68.onnx landmark_3d_68\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "model ignore: /home/jetsonuser/.insightface/models/buffalo_s/2d106det.onnx landmark_2d_106\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: /home/jetsonuser/.insightface/models/buffalo_s/det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "model ignore: /home/jetsonuser/.insightface/models/buffalo_s/genderage.onnx genderage\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "model ignore: /home/jetsonuser/.insightface/models/buffalo_s/w600k_mbf.onnx recognition\n",
      "set det-size: (2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "# ── RetinaFace Setup (replace Faster R-CNN) ───────────────────────────────\n",
    "import torch, cv2\n",
    "\n",
    "# how often to re-run the detector vs. track\n",
    "DETECT_EVERY = 1\n",
    "\n",
    "\n",
    "# ── Replacement detect_objects using Ultra-Light ──────────────────────────────\n",
    "DETECTION_CONFIDENCE_THRESHOLD = 0.3\n",
    "PRIVATE_OBJECT_CLASSES = ['person']  # used downstream by segment_all\n",
    "WINDOW_SIZE = 10\n",
    "DEPTH_THRESHOLD_MULTIPLIER = 75\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# initialize once at top-level\n",
    "retina_app = FaceAnalysis(name='buffalo_s', allowed_modules=['detection'])\n",
    "# ctx_id=0 for GPU, -1 for CPU\n",
    "retina_app.prepare(ctx_id=0 if torch.cuda.is_available() else -1,\n",
    "                   det_size=(2048,2048))\n",
    "\n",
    "\n",
    "\n",
    "def detect_objects(model, image_np, confidence_threshold=0.5, draw_boxes=False):\n",
    "    \"\"\"\n",
    "    image_np: BGR uint8 H×W×3\n",
    "    Returns list of {'box':[x1,y1,x2,y2],'score':…,'label':'person'}\n",
    "    or, if draw_boxes=True, returns the BGR image with boxes overlaid.\n",
    "    \"\"\"\n",
    "    # 1) run detection\n",
    "    faces = retina_app.get(image_np)\n",
    "    objects = []\n",
    "    for f in faces:\n",
    "        score = f.det_score\n",
    "        if score < confidence_threshold:\n",
    "            continue\n",
    "        x1,y1,x2,y2 = map(int, f.bbox)\n",
    "        objects.append({\"box\":[x1,y1,x2,y2],\n",
    "                        \"score\":float(score),\n",
    "                        \"label\":\"person\"})\n",
    "\n",
    "    if not draw_boxes:\n",
    "        return objects\n",
    "\n",
    "    # 2) draw\n",
    "    out = image_np.copy()\n",
    "    for obj in objects:\n",
    "        x1,y1,x2,y2 = obj[\"box\"]\n",
    "        s = obj[\"score\"]\n",
    "        cv2.rectangle(out, (x1,y1),(x2,y2), (255,0,0), 2)\n",
    "        cv2.putText(out, f\"{s:.2f}\", (x1, max(y1-5,0)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "    return out\n",
    "\n",
    "###############################################################################\n",
    "# 2) DEPTH PROFILE\n",
    "###############################################################################\n",
    "def calculate_depth_profile_of_box(depth_map, x1, y1, x2, y2, window_size=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Return { 'mean','std','threshold','box':[x1,y1,x2,y2] } or None if empty.\n",
    "    \"\"\"\n",
    "    half_window = window_size // 2\n",
    "    cx = (x1 + x2) // 2\n",
    "    cy = (y1 + y2) // 2\n",
    "\n",
    "    x_start = max(cx - half_window, 0)\n",
    "    x_end = min(cx + half_window + 1, depth_map.shape[1])\n",
    "    y_start = max(cy - half_window, 0)\n",
    "    y_end = min(cy + half_window + 1, depth_map.shape[0])\n",
    "\n",
    "    depth_window = depth_map[y_start:y_end, x_start:x_end]\n",
    "    depth_values = depth_window.flatten()\n",
    "    depth_values = depth_values[~np.isnan(depth_values)]\n",
    "    if depth_values.size == 0:\n",
    "        return None\n",
    "\n",
    "    depth_mean = float(np.mean(depth_values))\n",
    "    depth_std = float(np.std(depth_values))\n",
    "    depth_threshold = float(depth_std * DEPTH_THRESHOLD_MULTIPLIER)\n",
    "\n",
    "    return {\n",
    "        'mean': depth_mean,\n",
    "        'std': depth_std,\n",
    "        'threshold': depth_threshold,\n",
    "        'box': [x1,y1,x2,y2]\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 3) SEGMENT CHUNK\n",
    "###############################################################################\n",
    "\n",
    "def segment_person_from_box(depth_tensor, dprof, span=1):\n",
    "    \"\"\"\n",
    "    Similar to segment_person_from_profile_batch, but for a single bounding box\n",
    "    in just 1 frame's depth or multiple frames (X frames).\n",
    "    If depth_tensor: shape [X,H,W] or [H,W].\n",
    "    \"\"\"\n",
    "    if len(depth_tensor.shape) == 2:\n",
    "        # single frame => shape [H,W]\n",
    "        depth_tensor = depth_tensor.unsqueeze(0)  # => [1,H,W]\n",
    "\n",
    "    depth_mean = torch.tensor(dprof['mean'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    depth_thr  = torch.tensor(dprof['threshold'], device=depth_tensor.device, dtype=depth_tensor.dtype)\n",
    "    (x1,y1,x2,y2) = dprof['box']\n",
    "    y2 = int(y1 + span * (y2 - y1))\n",
    "\n",
    "\n",
    "    depth_diff = torch.abs(depth_tensor - depth_mean)\n",
    "    mask_batch = (depth_diff <= depth_thr).to(torch.uint8)\n",
    "\n",
    "    final_mask = torch.zeros_like(mask_batch)\n",
    "    _, H, W = depth_tensor.shape\n",
    "    x1_clamp = max(0, min(x1,W))\n",
    "    x2_clamp = max(0, min(x2,W))\n",
    "    y1_clamp = max(0, min(y1,H))\n",
    "    y2_clamp = max(0, min(y2,H))\n",
    "\n",
    "    if x2_clamp> x1_clamp and y2_clamp> y1_clamp:\n",
    "        final_mask[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp] = \\\n",
    "            mask_batch[:, y1_clamp:y2_clamp, x1_clamp:x2_clamp]\n",
    "\n",
    "    # return shape [H,W] if single frame\n",
    "    if final_mask.shape[0] == 1:\n",
    "        return final_mask[0]\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "def segment_all(depth_tensor, objects, depth_map, span):\n",
    "    \"\"\"\n",
    "    We create a combined mask of shape [H,W] = 1 for each person's bounding box,\n",
    "    EXCEPT we skip the public_box (which is the \"public\" person).\n",
    "    depth_tensor: shape [H,W], float on GPU\n",
    "    objects: detection results on CPU\n",
    "    public_box: (x1,y1,x2,y2) that we skip\n",
    "    depth_map: CPU 2D array for depth\n",
    "    Return: torch.uint8 mask [H,W], 1=private, 0=public\n",
    "    \"\"\"\n",
    "    H, W = depth_tensor.shape[-2], depth_tensor.shape[-1]\n",
    "\n",
    "    combined_mask = torch.zeros((H,W), dtype=torch.uint8, device=depth_tensor.device)\n",
    "\n",
    "    for obj in objects:\n",
    "        if obj['label'] not in PRIVATE_OBJECT_CLASSES:\n",
    "            continue\n",
    "        box = obj['box']  # [x1,y1,x2,y2]\n",
    "\n",
    "        # otherwise, segment this bounding box => \"private\"\n",
    "        dprof = calculate_depth_profile_of_box(depth_map, *box)\n",
    "        if dprof is None:\n",
    "            continue\n",
    "\n",
    "        single_mask = segment_person_from_box(depth_tensor, dprof, span)\n",
    "        combined_mask = torch.logical_or(combined_mask.bool(), single_mask.bool()).to(torch.uint8)\n",
    "\n",
    "    return combined_mask\n",
    "\n",
    "###############################################################################\n",
    "# 4) METRICS\n",
    "###############################################################################\n",
    "def dice_score_batch(pred_batch, gt_batch):\n",
    "    intersection = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    pred_sum = torch.sum(pred_batch==1).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if (pred_sum+gt_sum)==0:\n",
    "        return 1.0\n",
    "    return 2.0*intersection/(pred_sum+gt_sum)\n",
    "\n",
    "def recall_batch(pred_batch, gt_batch):\n",
    "    tp = torch.sum((pred_batch==1)&(gt_batch==1)).item()\n",
    "    gt_sum = torch.sum(gt_batch==1).item()\n",
    "    if gt_sum==0:\n",
    "        return 1.0\n",
    "    return tp/gt_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3452cd-4e47-49cc-b77d-d2f7af4945d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk starts: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "Detection times per chunk: [3.424869182000009, 0.3710870760000091, 0.20293396099998517, 0.2001912300000015, 0.17679713199999014, 0.16320440999999164, 0.1646034919999977, 0.15684660399998052, 0.15411532999999622, 0.1682308750000061, 0.15350998200000276, 0.16033555000001343, 0.15174066900002003, 0.1516131479999956, 0.14121625600000698, 0.15015902400000414, 0.1336378800000091, 0.14921481100000733, 0.15556424700000093, 0.14407102399999872, 0.14639678599999684, 0.13658874300000434, 0.13599322100000677, 0.13760815500000945, 0.13831312500002468, 0.13293125700002406, 0.1374674169999821, 0.13590930200001594, 0.14133874500001298, 0.13486499299997945, 0.13645429499999295, 0.13470182300000033, 0.13528701500001716, 0.13280463699999245, 0.13919280900000786, 0.14449083100001303, 0.1337806070000056, 0.14664209200000755, 0.14015675699999974, 0.13898722499999394, 0.15186998000001495, 0.1544048610000175, 0.152079069999985, 0.13641822599998932, 0.14970031900000436, 0.1346081849999905, 0.13914595700001087, 0.13949626499999113, 0.1478466109999772, 0.15122553399999106, 0.13376380799999765, 0.13760075099997948, 0.13549360899997964, 0.13855729499999825, 0.1371900450000112, 0.13867064100000448, 0.14194428100000778, 0.13840293600000564, 0.1406938180000168, 0.14134979499999645, 0.14249758500000098, 0.1387332509999908, 0.1391868890000012, 0.13989171800000122, 0.1391608280000014, 0.13862027799999055, 0.13370194800000945, 0.13914089200000035, 0.13811611299999527, 0.145516262000001, 0.13916200199997775, 0.137576896000013, 0.14404788100000587, 0.13759999900000253, 0.1421244349999995, 0.13714863500001684, 0.14270367299999975, 0.13823206199998594, 0.14269233599998188, 0.13946951500000182]\n",
      "Segmentation times per chunk: [0.250073059000016, 0.020699495999991768, 0.02274058399999035, 0.021243088000005628, 0.013445523999990883, 0.021291560000008758, 0.018062294000003476, 0.04324325700000031, 0.015719825000019227, 0.025076898000008896, 0.019471906999996236, 0.012638813000023674, 0.027590715999991744, 0.008638946000019132, 0.00964699200000041, 0.00849919500001306, 0.010881853999990199, 0.011111330000005637, 0.007202409000001353, 0.0060532720000026075, 0.007676784000011594, 0.008619929999980513, 0.01282799000000523, 0.009328484000008075, 0.0024821789999975863, 0.009458310000013626, 0.012075723000009475, 0.008704311999991887, 0.011221946000006255, 0.014960814000005485, 0.005870960999999397, 0.01325154199997769, 0.017793268999980683, 0.01229389299999184, 0.008251150000006646, 0.01702707600000508, 0.021416670999997223, 0.017315592000016977, 0.010307850000003782, 0.01943295700002068, 0.01993440399999713, 0.01683465099998216, 0.004607931999998982, 0.006817208999990498, 0.0023983670000120583, 0.006105101999992257, 0.0062542869999901995, 0.020512867999997297, 0.0071735310000065056, 0.004764125000008335, 0.005711208000008128, 0.005020286000018359, 0.005017470000012736, 0.005172799999996869, 0.004749083000007204, 0.005117855000008831, 0.004870044000000462, 0.004425397999995084, 0.005073213000002852, 0.005693059999998695, 0.005164541999988614, 0.00461656699999935, 0.005069404999971994, 0.004972089999995433, 0.0032825019999904725, 0.0034506009999972775, 0.0024896609999984776, 0.002253050000007306, 0.002645695000012438, 0.0025203499999975065, 0.003482920000010381, 0.002724638999978879, 0.0027413749999993797, 0.002751231999980064, 0.0026358060000006844, 0.002577149000018153, 0.007305840999975999, 0.0026758380000160287, 0.0024573709999913262, 0.0027502060000017536]\n",
      "Total times per chunk: [3.862286855999997, 0.41703946100000167, 0.24006456499998308, 0.2412799120000102, 0.20489024900001596, 0.20175876999999787, 0.19814723900000786, 0.21612185099999692, 0.18967414899998403, 0.20819030500001645, 0.18711666699999796, 0.18935060799998382, 0.19865087499999845, 0.17389888900001438, 0.172230030999998, 0.17200702600001705, 0.15798034700000585, 0.18073472499997933, 0.18349342100000854, 0.16342836100000113, 0.17308101399999032, 0.15867158800000425, 0.1626484459999915, 0.16494671800001015, 0.15403206700000283, 0.15575564399998143, 0.162988385999995, 0.16114402399998085, 0.16820973999998046, 0.16613857499999085, 0.1581154889999823, 0.16338212100001215, 0.1687346429999934, 0.16288648700000863, 0.1646243810000101, 0.17869842600001107, 0.168632051000003, 0.17721773399998142, 0.16377768099999912, 0.1719055780000076, 0.19351980300001514, 0.1926454549999903, 0.1775067940000099, 0.1580448120000142, 0.17048161399998207, 0.15571284100002458, 0.16119545199998697, 0.17866751500000078, 0.17476580099997818, 0.17513668600000187, 0.15710202500000037, 0.15839330000000018, 0.15821363400002042, 0.15860848599999144, 0.15951163300002236, 0.1586759109999889, 0.16458961600000066, 0.16041728600001193, 0.16042266399998084, 0.16459781900002213, 0.1669166300000029, 0.16189545000000294, 0.15904427899999973, 0.16261032500000283, 0.16012430600000016, 0.15755051499999695, 0.1526503949999949, 0.15623165200000244, 0.15555618799999138, 0.16585739699999635, 0.1604905990000134, 0.15669634599998972, 0.16434235200000558, 0.15822065100002192, 0.16234748200000126, 0.15746624199999815, 0.1678910290000033, 0.16115398100001244, 0.1628328460000148, 0.15702628599999002]\n",
      "Avg detection time     = 0.1884s\n",
      "Avg segmentation time  = 0.0129s\n",
      "Avg total time/chunk   = 0.2201s\n",
      "Total time all chunks  = 17.6090s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Allocate mask tensor [V, F, H, W]\n",
    "pred_mask_full = torch.zeros(\n",
    "    (num_views, num_frames, images.shape[-2], images.shape[-1]),\n",
    "    dtype=torch.uint8,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "chunk_detection_times = []\n",
    "chunk_seg_times       = []\n",
    "chunk_total_times     = []\n",
    "\n",
    "chunk_starts = list(range(0, num_frames, X))\n",
    "print(f\"Chunk starts: {chunk_starts}\")\n",
    "\n",
    "for start_f in chunk_starts:\n",
    "    end_f = min(start_f + X, num_frames)\n",
    "    t_chunk = time.perf_counter()\n",
    "\n",
    "    for v in range(num_views):\n",
    "        # 1) Prepare the BGR image for this view/frame\n",
    "        rgb_t = images[v, start_f, :3]                           # [3,H,W] float[0–1]\n",
    "        rgb_np = (rgb_t.permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "        bgr_np = cv2.cvtColor(rgb_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        det_start = time.perf_counter()\n",
    "        # 2) Decide: detect fresh or update tracker\n",
    "        if start_f % DETECT_EVERY == 0:\n",
    "            # clear & re-seed the tracker\n",
    "            dets = detect_objects(None, bgr_np, DETECTION_CONFIDENCE_THRESHOLD)\n",
    "            boxes = []\n",
    "            for det in dets:\n",
    "                x1,y1,x2,y2 = det['box']\n",
    "                w, h = x2-x1, y2-y1\n",
    "                # add a new CSRT tracker for this box\n",
    "                boxes.append([x1,y1,x2,y2])\n",
    "\n",
    "        chunk_detection_times.append(time.perf_counter() - det_start)\n",
    "\n",
    "        # 3) Build a simple list for segmentation:\n",
    "        dets_for_seg = [{'box':b,'label':'person'} for b in boxes]\n",
    "\n",
    "        # 4) Depth‐segment unchanged\n",
    "        depth_map = images[v, start_f, 3].cpu().numpy()\n",
    "        depth_chunk = images[v, start_f:end_f, 3]  # [X,H,W]\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        private_mask = segment_all(depth_chunk, dets_for_seg, depth_map, mask_height_span)\n",
    "        chunk_seg_times.append(time.perf_counter() - t0)\n",
    "\n",
    "        # 5) Store your mask\n",
    "        pred_mask_full[v, start_f:end_f] = private_mask\n",
    "\n",
    "    chunk_total_times.append(time.perf_counter() - t_chunk)\n",
    "\n",
    "\n",
    "# Print out timing stats\n",
    "print(\"Detection times per chunk:\", chunk_detection_times)\n",
    "print(\"Segmentation times per chunk:\", chunk_seg_times)\n",
    "print(\"Total times per chunk:\", chunk_total_times)\n",
    "\n",
    "# (Optional) Compute metrics if using full-body masks\n",
    "# if mask_height_span > .8:\n",
    "#     dice = recall = 0\n",
    "#     for v in range(num_views):\n",
    "#         gt = (masks[v]==2).to(torch.uint8)\n",
    "#         d  = dice_score_batch(pred_mask_full[v].cpu(), gt.cpu())\n",
    "#         r  = recall_batch(pred_mask_full[v].cpu(), gt.cpu())\n",
    "#         dice += d; recall += r\n",
    "#         print(f\"View {v}: Dice={d:.4f}, Recall={r:.4f}\")\n",
    "#     print(\"Avg Dice:\", dice/num_views, \"Avg Recall:\", recall/num_views)\n",
    "\n",
    "# Overall timing\n",
    "num_chunks = len(chunk_starts)\n",
    "avg_det = sum(chunk_detection_times)/len(chunk_detection_times)\n",
    "avg_seg = sum(chunk_seg_times)/num_chunks\n",
    "avg_tot = sum(chunk_total_times)/num_chunks\n",
    "\n",
    "print(f\"Avg detection time     = {avg_det:.4f}s\")\n",
    "print(f\"Avg segmentation time  = {avg_seg:.4f}s\")\n",
    "print(f\"Avg total time/chunk   = {avg_tot:.4f}s\")\n",
    "print(f\"Total time all chunks  = {sum(chunk_total_times):.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eecbf03-3d50-4ac1-a290-344ad7c255c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "print(pred_mask_full[v].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8491da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def anonymize_region_gpu(img_tensor: torch.Tensor,\n",
    "                             mask_tensor: torch.Tensor,\n",
    "                             block: int = 16,\n",
    "                             noise_level: int = 25) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Optimized GPU anonymization: pixelates + noise on the masked region.\n",
    "    Works without padding errors by extracting the ROI mask.\n",
    "    \"\"\"\n",
    "    # 1) Prepare image in float [0,1]\n",
    "    orig_dtype = img_tensor.dtype\n",
    "    img = img_tensor.float().to(img_tensor.device)\n",
    "    if orig_dtype == torch.uint8:\n",
    "        img = img / 255.0\n",
    "\n",
    "    # 2) Find ROI bounds from full-image mask\n",
    "    ys, xs = mask_tensor.nonzero(as_tuple=True)\n",
    "    if ys.numel() == 0:\n",
    "        return img_tensor  # nothing to anonymize\n",
    "\n",
    "    y1, y2 = ys.min().item(), ys.max().item() + 1\n",
    "    x1, x2 = xs.min().item(), xs.max().item() + 1\n",
    "\n",
    "    # 3) Crop ROI from image and mask\n",
    "    roi = img[:, y1:y2, x1:x2]               # [C, h, w]\n",
    "    mask_roi = mask_tensor[y1:y2, x1:x2]     # [h, w]\n",
    "    C, h, w = roi.shape\n",
    "\n",
    "    # 4) Pixelate via avg pool → nearest upsample (ceil_mode handles edges)\n",
    "    pooled = F.avg_pool2d(\n",
    "        roi.unsqueeze(0),               # [1, C, h, w]\n",
    "        kernel_size=block,\n",
    "        stride=block,\n",
    "        ceil_mode=True\n",
    "    )                                   # → [1, C, ceil(h/block), ceil(w/block)]\n",
    "    mosaic = F.interpolate(\n",
    "        pooled,\n",
    "        size=(h, w),\n",
    "        mode='nearest'\n",
    "    ).squeeze(0)                        # → [C, h, w]\n",
    "\n",
    "    # 5) Add uniform noise to mosaic blocks\n",
    "    noise = (torch.rand_like(mosaic) * 2 - 1) * (noise_level / 255.0)\n",
    "    mosaic_noised = (mosaic + noise).clamp(0.0, 1.0)\n",
    "\n",
    "    # 6) Blend only the masked pixels in the ROI\n",
    "    mask_f = mask_roi.to(dtype=mosaic_noised.dtype, device=img.device)      # [h, w]\n",
    "    mask_exp = mask_f.unsqueeze(0).expand(C, h, w)                         # [C, h, w]\n",
    "    region = mosaic_noised * mask_exp + roi * (1.0 - mask_exp)             # [C, h, w]\n",
    "\n",
    "    # 7) Write region back into a copy of the full image\n",
    "    out = img.clone()\n",
    "    out[:, y1:y2, x1:x2] = region\n",
    "\n",
    "    # 8) Convert back to uint8 if needed\n",
    "    if orig_dtype == torch.uint8:\n",
    "        out = (out * 255.0).round().to(torch.uint8)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def anonymize_depth(depth_np: np.ndarray, mask: np.ndarray = None, noise_strength: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add noise to depth data to anonymize the region. Only applies to mask area if mask is given.\n",
    "    depth_np: H×W depth array (float32).\n",
    "    mask: H×W boolean array for region to anonymize (same size as depth_np).\n",
    "    \"\"\"\n",
    "    depth_out = depth_np.copy()\n",
    "    if mask is None:\n",
    "        # If no mask provided, apply to whole depth (not recommended for performance)\n",
    "        mask = np.ones_like(depth_out, dtype=bool)\n",
    "    # You can add just one type of noise for speed. Here we use Gaussian.\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_strength, size=depth_out.shape)\n",
    "    depth_out[mask] += noise[mask]\n",
    "    # Optionally clamp or otherwise limit values if needed (e.g., keep depth in plausible range).\n",
    "    return depth_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca63f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simple_lama_inpainting'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msimple_lama_inpainting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleLama\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simple_lama_inpainting'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from simple_lama_inpainting import SimpleLama\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tqdm\n",
    "# from toonify_image import load_toonify_model, toonify_image_with_stylegan\n",
    "\n",
    "\n",
    "\n",
    "PSP_MODEL_PATH = \"pretrained_models/psp_toonify.pt\"\n",
    "\n",
    "\n",
    "model_used = True\n",
    "model_used = False\n",
    "\n",
    "if model_used:\n",
    "    simple_lama = SimpleLama()\n",
    "\n",
    "    # Load stable_diffusion from Hugging Face\n",
    "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-inpainting\",\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",  # necessary for speed\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # # Load SSD-1B from Hugging Face\n",
    "    # pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    #     \"\",\n",
    "    #     torch_dtype=torch.float16,\n",
    "    #     variant=\"fp16\",  # necessary for speed\n",
    "    # ).to(\"cuda\")\n",
    "\n",
    "    # load style GAN\n",
    "    loaded_toonify_model = load_toonify_model(PSP_MODEL_PATH)\n",
    "    print(\"Toonify model successfully loaded for the pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a999ea37-5815-4083-b087-d0f4e59e4a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-chunk totals: [0.2666816180000069, 0.2483658430000446, 0.2472428129999571, 0.24681783800002677, 0.24984224699994684, 0.24551509299999452, 0.23384696800002303, 0.23453945899996143, 0.23652621299999055, 0.23745016700001997, 0.2341406009999787, 0.23381781099999444, 0.24263909900003, 0.22505716300003087, 0.23123763099999906, 0.23344131900000775, 0.22425789900000836, 0.24006031900000835, 0.23321379200001502, 0.23470348400002194, 0.22576135899998917, 0.22426242400001684, 0.22529642700004615, 0.22467167999997173, 0.22483725099999674, 0.22480740200001037, 0.22523428099998455, 0.22477164899999025, 0.2245036760000403, 0.22383485399996061, 0.23408682499996303, 0.23007052799999883, 0.22589330099998506, 0.22588376400000243, 0.22496071000000484, 0.22630329200001142, 0.22681219699995836, 0.22681834099995513, 0.22618244100004858, 0.22673229200000833, 0.2268931899999984, 0.22616796099998737, 0.2369619739999962, 0.24066447400002744, 0.2254447500000083, 0.22706880999999157, 0.2278524070000003, 0.22741371999995863, 0.2267361969999797, 0.32828994499999453, 0.378711294000027, 0.2346583849999888, 0.23579539700000396, 0.23995658899997352, 0.23973772900001222, 0.23626657299996623, 0.2271040469999548, 0.23373331400000552, 0.2308319189999679, 0.22876267099996994, 0.23289986100002125, 0.22998198699997374, 0.2285152420000145, 0.22923478900003147, 0.22306766399998423, 0.22276311500002066, 0.23518761399998311, 0.23318370100002994, 0.23112972399997034, 0.2303473100000133, 0.23250206700004128, 0.22261550100000704, 0.2225065790000258, 0.22235428499999443, 0.22235415699998384, 0.22990855499995178, 0.2302109799999812, 0.2269852780000292, 0.22656603900003347, 0.2271707539999852]\n",
      "Per-chunk anonym times: [0.03878247099999044, 0.029171919000020807, 0.029071117000000868, 0.025845199000002594, 0.027577259999986836, 0.026822781000021223, 0.017335785000000214, 0.018271227000013823, 0.01841881000001422, 0.019773171999986516, 0.017057409000017287, 0.017188996000015777, 0.023766112000032535, 0.013289783999994143, 0.02064992200001825, 0.021604980000006435, 0.013168629999995574, 0.01975857399997949, 0.021095590000015818, 0.020060659999955988, 0.013775711999983287, 0.01302539099998512, 0.013355828999976893, 0.012994703000003938, 0.013217875000009371, 0.013102800000012849, 0.013019052999993619, 0.012998635999963426, 0.013008877000004304, 0.012972748999970918, 0.022143889000005856, 0.014284194999959254, 0.014297442000042793, 0.014467109999998229, 0.013051596000025256, 0.01457946099998253, 0.01446326699999645, 0.015000781000026109, 0.014834313999983806, 0.014828872000009596, 0.015239215999997668, 0.014875464999988708, 0.026162514000020565, 0.028795005000006313, 0.013558927999952175, 0.01525697400001036, 0.015658035999990716, 0.015514673999973638, 0.015646897999999965, 0.015426190000027873, 0.022897010000008322, 0.02331195699997579, 0.024243333999947936, 0.027902885000003153, 0.02769772900001044, 0.024226913999996214, 0.016196054999966236, 0.018862214000023414, 0.01915533799996183, 0.017509067999981198, 0.021679796000000806, 0.019025574000011147, 0.017972148000012567, 0.01858275000000731, 0.013151040000025205, 0.012934012000016537, 0.021064741999964554, 0.022912036999969132, 0.018122000999994725, 0.020290934000001926, 0.013712711999971816, 0.013061596000000009, 0.012986362999981793, 0.013090779000037855, 0.013117371000021194, 0.015670630999977675, 0.017316193999988627, 0.016100076000043373, 0.015746277999994618, 0.01292907900000273]\n",
      "Per-chunk write times: [0.18732381099999884, 0.1871856000000207, 0.18594189599997435, 0.18902239599998438, 0.18961633599997185, 0.18636341000001266, 0.18480488500000547, 0.18467841700004328, 0.18629483600000185, 0.1858948279999595, 0.18557763799998384, 0.18470373399998152, 0.18563364300001695, 0.1797875820000172, 0.17948827999998684, 0.1800152009999465, 0.17973738800003503, 0.18920926499998814, 0.18038954899998316, 0.18310422399997606, 0.1807197890000225, 0.1802628959999879, 0.1807304879999947, 0.18051749200003542, 0.18045217899998534, 0.18053874800000358, 0.18101078699999107, 0.1805151300000034, 0.18027326899999707, 0.17969865800000662, 0.1805811209999888, 0.18471719600000824, 0.180371708999985, 0.1802867479999577, 0.18054695099999662, 0.1803467550000164, 0.1807997870000122, 0.18048515800001041, 0.17992500299999392, 0.18051550100000213, 0.17992768199997045, 0.1801962950000302, 0.17961634400001003, 0.1806793510000375, 0.18071317499999395, 0.18056386000000657, 0.180661941999972, 0.1807105269999738, 0.17969454100000348, 0.28156922199997325, 0.32398979099997405, 0.17990789699996412, 0.1800769550000041, 0.1794894250000425, 0.17966237999996792, 0.18065751799997543, 0.179267445999983, 0.18352646300002107, 0.18037453700003425, 0.17987866499998972, 0.17961952399997472, 0.1794263380000416, 0.17928415900001937, 0.17894256699997868, 0.17900467599997683, 0.17881004800000255, 0.1806187829999999, 0.17909878099999332, 0.17928571399994553, 0.17841353800002935, 0.18759484600002452, 0.17842553900004532, 0.17816816900000276, 0.1781868890000169, 0.1781053189999966, 0.1823768790000031, 0.18120289500001263, 0.1796900919999871, 0.17976487700002508, 0.18276473600002419]\n",
      "Avg total: 0.2342457327374987\n",
      "Avg anon: 0.018121693337498357\n",
      "Avg write: 0.18447520836249892\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def save_masked_images_gpu(pred_mask_full, images, out_folder, dilation_radius=4):\n",
    "    \"\"\"\n",
    "    Saves masked RGB & depth frames and records timing.\n",
    "    Returns dict with lists: chunk_total, chunk_anon, chunk_write.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    V, F, H, W = pred_mask_full.shape\n",
    "\n",
    "    # CPU kernel for dilation\n",
    "    k = 2 * dilation_radius + 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))\n",
    "\n",
    "    # Make view folders\n",
    "    for v in range(V):\n",
    "        os.makedirs(os.path.join(out_folder, \"rgb\",   f\"view{v}\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(out_folder, \"depth\", f\"view{v}\"), exist_ok=True)\n",
    "\n",
    "    chunk_total_times = []\n",
    "    chunk_anonym_times = []\n",
    "    chunk_write_times = []\n",
    "\n",
    "    # Process in chunks of X frames\n",
    "    for start in range(0, F, X):\n",
    "        # ensure all prior CUDA work is done\n",
    "        torch.cuda.synchronize()\n",
    "        t_chunk_start = time.perf_counter()\n",
    "\n",
    "        anon_times = []\n",
    "        write_times = []\n",
    "        end = min(start + X, F)\n",
    "\n",
    "        for v in range(V):\n",
    "            for f in range(start, end):\n",
    "                # -- 1) dilate mask on CPU (fast) --\n",
    "                mask_np = pred_mask_full[v, f].cpu().numpy().astype(np.uint8)\n",
    "                mask_cpu = cv2.dilate(mask_np, kernel).astype(bool)\n",
    "\n",
    "                # -- 2) grab depth for saving (so we don't time I/O here) --\n",
    "                depth = images[v, f, 3].cpu().numpy()\n",
    "\n",
    "                # -- 3) move tensors to GPU --\n",
    "                rgb_gpu  = images[v, f, :3]  # [3,H,W]\n",
    "                mask_gpu = torch.from_numpy(mask_cpu).to(rgb_gpu.device)  # [H,W]\n",
    "\n",
    "                # -- 4) anonymize once per chunk --\n",
    "                if f == start:\n",
    "                    # sync to measure full GPU kernel time\n",
    "                    torch.cuda.synchronize()\n",
    "                    t_anon_start = time.perf_counter()\n",
    "\n",
    "                    anon_gpu = anonymize_region_gpu(\n",
    "                        rgb_gpu, mask_gpu,\n",
    "                        # method=\"fast_mosaic\",\n",
    "                        block=max(1, W//16),\n",
    "                        noise_level=20\n",
    "                    )\n",
    "\n",
    "                    # bring back to CPU uint8 H×W×3 (this syncs too)\n",
    "                    anon_arr = (\n",
    "                        anon_gpu.permute(1,2,0)\n",
    "                                 .cpu()\n",
    "                                 .numpy()\n",
    "                    )\n",
    "                    if anon_arr.dtype != np.uint8:\n",
    "                        anon_arr = (anon_arr * 255).clip(0,255).astype(np.uint8)\n",
    "\n",
    "                    anon_times.append(time.perf_counter() - t_anon_start)\n",
    "\n",
    "                # -- 5) composite --\n",
    "                orig = rgb_gpu.permute(1,2,0).cpu().numpy()\n",
    "                if orig.dtype != np.uint8:\n",
    "                    orig = (orig * 255).clip(0,255).astype(np.uint8)\n",
    "                out_rgb = orig.copy()\n",
    "                out_rgb[mask_cpu] = anon_arr[mask_cpu]\n",
    "\n",
    "                # -- 6) write & time it --\n",
    "                t_write_start = time.perf_counter()\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(out_folder, \"rgb\", f\"view{v}\", f\"{f}_masked.png\"),\n",
    "                    cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR)\n",
    "                )\n",
    "                depth_u16 = np.clip(depth + np.random.normal(0,10,depth.shape),\n",
    "                                     0,20000).astype(np.uint16)\n",
    "                cv2.imwrite(\n",
    "                    os.path.join(out_folder, \"depth\", f\"view{v}\", f\"{f}_depth.png\"),\n",
    "                    depth_u16\n",
    "                )\n",
    "                write_times.append(time.perf_counter() - t_write_start)\n",
    "\n",
    "        # ensure all CUDA work (and any copies) finish before stopping the chunk timer\n",
    "        torch.cuda.synchronize()\n",
    "        chunk_total_times.append(time.perf_counter() - t_chunk_start)\n",
    "        chunk_anonym_times.append(sum(anon_times))\n",
    "        chunk_write_times.append(sum(write_times))\n",
    "\n",
    "    return {\n",
    "        \"chunk_total\": chunk_total_times,\n",
    "        \"chunk_anon\":  chunk_anonym_times,\n",
    "        \"chunk_write\": chunk_write_times\n",
    "    }\n",
    "\n",
    "# Usage:\n",
    "timings = save_masked_images_gpu(\n",
    "    pred_mask_full, images, output_base_directory, dilation_radius=4\n",
    ")\n",
    "print(\"Per-chunk totals:\", timings[\"chunk_total\"])\n",
    "print(\"Per-chunk anonym times:\", timings[\"chunk_anon\"])\n",
    "print(\"Per-chunk write times:\", timings[\"chunk_write\"])\n",
    "print(\"Avg total:\", np.mean(timings[\"chunk_total\"]))\n",
    "print(\"Avg anon:\",  np.mean(timings[\"chunk_anon\"]))\n",
    "print(\"Avg write:\", np.mean(timings[\"chunk_write\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11fb79be-015a-4807-9e1a-0eacf76be0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg anonymization time per chunk: 0.0193s\n"
     ]
    }
   ],
   "source": [
    "# ── Run the GPU‐powered save function ─────────────────────────────────────────\n",
    "avg_anon = np.mean(timings['chunk_anon'])\n",
    "print(f\"Avg anonymization time per chunk: {avg_anon:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c31f079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All detection‐overlay images saved under: output/carla_v1/rgb_detect\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Paths\n",
    "base = output_base_directory                  # e.g. \"output/xr_lubna\"\n",
    "mask_root = os.path.join(base, \"rgb\")         # your masked imgs: rgb/view0/0_masked.png...\n",
    "det_root  = os.path.join(base, \"rgb_detect\")\n",
    "\n",
    "os.makedirs(det_root, exist_ok=True)\n",
    "\n",
    "for v in range(num_views):\n",
    "    mask_dir = os.path.join(mask_root, f\"view{v}\")\n",
    "    det_dir  = os.path.join(det_root,  f\"view{v}\")\n",
    "    os.makedirs(det_dir, exist_ok=True)\n",
    "    \n",
    "    for f in range(num_frames):\n",
    "        mask_path = os.path.join(mask_dir, f\"{f}_masked.png\")\n",
    "        img = cv2.imread(mask_path)\n",
    "        if img is None:\n",
    "            print(f\"⚠️ Missing frame {mask_path}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Run your detector in BGR uint8\n",
    "        # (switch to detect_fast_scrfd if you prefer SCRFD)\n",
    "        det_img = detect_objects(None, img,\n",
    "                                 confidence_threshold=DETECTION_CONFIDENCE_THRESHOLD,\n",
    "                                 draw_boxes=True)\n",
    "\n",
    "        out_path = os.path.join(det_dir, f\"{f}_detect.png\")\n",
    "        cv2.imwrite(out_path, det_img)\n",
    "\n",
    "print(\"✅ All detection‐overlay images saved under:\", det_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55031995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated FPS: 31.11 frames per second\n"
     ]
    }
   ],
   "source": [
    "fps = 1/((avg_anon + avg_seg)/ X)\n",
    "print(f\"Estimated FPS: {fps:.2f} frames per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60b117b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to output/carla_v1/videos/view0.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# fps=30\n",
    "# Set parameters\n",
    "num_frames = num_frames  # Already defined in your notebook\n",
    "\n",
    "os.makedirs(video_output_dir, exist_ok=True)\n",
    "for view_idx in range(num_views):\n",
    "    output_path = os.path.join(video_output_dir, f\"view{view_idx}.mp4\")\n",
    "    \n",
    "    input_video_pattern = input_video_base_path + f\"{view_idx}/{{}}_masked.png\"\n",
    "    # input_video_pattern = input_video_base_path + f\"{view_idx}/{{}}_detect.png\"\n",
    "\n",
    "    # Read the first frame to get the size\n",
    "    first_frame_path = input_video_pattern.format(0)\n",
    "    first_frame = cv2.imread(first_frame_path)\n",
    "    if first_frame is None:\n",
    "        raise FileNotFoundError(f\"First frame not found: {first_frame_path}\")\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    # Define the video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write frames to video\n",
    "    for i in range(num_frames):\n",
    "        frame_path = input_video_pattern.format(i)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            print(f\"Warning: Frame not found: {frame_path}, skipping.\")\n",
    "            continue\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f49bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb8ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2adde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jetson-pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
